"""
전량 처리 모듈 (16만건 처리)
"""
import asyncio
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
import logging
import math

from app.models.document import (
    BatchJob, DocumentCase, ProcessingResult, QualityMetrics,
    ProcessingStatus, ProcessingMode
)
from app.core.config import settings
from app.core.database import document_repo, result_repo, cache_manager
from app.services.openai_service import OpenAIService
from app.services.dsl_rules import dsl_manager

logger = logging.getLogger(__name__)


class FullProcessor:
    """전량 처리기"""
    
    def __init__(self):
        self.openai_service = OpenAIService()
        self.current_job: Optional[BatchJob] = None
        self.processing_stats = {
            "total_cases": 0,
            "processed_cases": 0,
            "failed_cases": 0,
            "start_time": None,
            "estimated_completion": None,
            "current_batch": 0,
            "total_batches": 0
        }
    
    async def start_full_processing(
        self, 
        processing_options: Dict[str, Any]
    ) -> Dict[str, Any]:
        """전량 처리 시작 (수동 버튼으로만 시작)"""
        
        try:
            logger.info("Starting full processing (160,000 cases)")
            
            # 전환 조건 확인
            readiness_check = await self._check_readiness_conditions()
            if not readiness_check["ready"]:
                raise ValueError(f"Not ready for full processing: {readiness_check['reason']}")
            
            # 1% 드라이런 실행 (선택사항)
            if processing_options.get("run_dry_run", True):
                dry_run_result = await self._execute_dry_run()
                if not dry_run_result["success"]:
                    raise ValueError(f"Dry run failed: {dry_run_result['reason']}")
            
            # 전량 처리 작업 생성
            batch_job = await self._create_full_processing_job(processing_options)
            self.current_job = batch_job
            
            # 처리 실행
            processing_result = await self._execute_full_processing(batch_job)
            
            return {
                "job_id": batch_job.job_id,
                "status": "started",
                "processing_options": processing_options,
                "estimated_duration_hours": processing_result.get("estimated_duration_hours"),
                "estimated_cost": processing_result.get("estimated_cost"),
                "total_cases": batch_job.total_cases
            }
            
        except Exception as e:
            logger.error(f"Failed to start full processing: {e}")
            raise
    
    async def _check_readiness_conditions(self) -> Dict[str, Any]:
        """전환 조건 확인"""
        
        try:
            # 1. 홀드아웃/대량 샘플 합격선 확인
            latest_batch_results = await self._get_latest_batch_results()
            if not latest_batch_results:
                return {"ready": False, "reason": "No recent batch results available"}
            
            quality_passed = self._check_quality_gates_passed(latest_batch_results)
            if not quality_passed:
                return {"ready": False, "reason": "Quality gates not passed in recent batch"}
            
            # 2. 회귀 테스트 확인
            regression_check = await self._check_recent_regressions()
            if not regression_check["passed"]:
                return {"ready": False, "reason": f"Recent regressions detected: {regression_check['count']}"}
            
            # 3. 규칙 안정성 확인
            stability_check = await self._check_rules_stability()
            if not stability_check["stable"]:
                return {"ready": False, "reason": f"Rules not stable: {stability_check['reason']}"}
            
            return {"ready": True, "reason": "All conditions met"}
            
        except Exception as e:
            logger.error(f"Failed to check readiness conditions: {e}")
            return {"ready": False, "reason": f"Check failed: {str(e)}"}
    
    async def _execute_dry_run(self) -> Dict[str, Any]:
        """1% 드라이런 실행 (약 1,600건)"""
        
        try:
            logger.info("Starting 1% dry run (1,600 cases)")
            
            dry_run_size = 1600
            start_time = datetime.now()
            
            # 샘플 선택
            sample_cases = await document_repo.get_stratified_sample({}, dry_run_size)
            
            if len(sample_cases) < dry_run_size * 0.9:  # 90% 이상 확보
                return {
                    "success": False,
                    "reason": f"Insufficient sample size: {len(sample_cases)} < {dry_run_size * 0.9}"
                }
            
            # 처리 실행
            processed_count = 0
            failed_count = 0
            total_processing_time = 0
            
            # 배치 단위로 처리
            batch_size = min(100, len(sample_cases))
            batches = [sample_cases[i:i + batch_size] for i in range(0, len(sample_cases), batch_size)]
            
            for batch in batches:
                batch_start = datetime.now()
                
                # 병렬 처리
                batch_results = await self._process_batch_parallel(batch)
                
                batch_end = datetime.now()
                batch_time = (batch_end - batch_start).total_seconds()
                total_processing_time += batch_time
                
                # 결과 집계
                for result in batch_results:
                    if result.get("success", False):
                        processed_count += 1
                    else:
                        failed_count += 1
            
            end_time = datetime.now()
            total_duration = (end_time - start_time).total_seconds()
            
            # 성능 지표 계산
            avg_processing_time_per_case = total_processing_time / len(sample_cases)
            failure_rate = failed_count / len(sample_cases)
            
            # 전량 처리 예상치 계산
            estimated_full_duration_hours = (160000 * avg_processing_time_per_case) / 3600
            estimated_cost = await self._estimate_full_processing_cost(avg_processing_time_per_case)
            
            # 성공 조건 확인
            success = (
                failure_rate <= 0.05 and  # 실패율 5% 이하
                avg_processing_time_per_case <= 10 and  # 케이스당 10초 이하
                estimated_cost <= 5000  # 예상 비용 $5,000 이하
            )
            
            result = {
                "success": success,
                "processed_count": processed_count,
                "failed_count": failed_count,
                "failure_rate": failure_rate,
                "avg_processing_time_per_case": avg_processing_time_per_case,
                "total_duration_seconds": total_duration,
                "estimated_full_duration_hours": estimated_full_duration_hours,
                "estimated_cost": estimated_cost
            }
            
            if not success:
                reasons = []
                if failure_rate > 0.05:
                    reasons.append(f"High failure rate: {failure_rate:.2%}")
                if avg_processing_time_per_case > 10:
                    reasons.append(f"Slow processing: {avg_processing_time_per_case:.2f}s per case")
                if estimated_cost > 5000:
                    reasons.append(f"High cost: ${estimated_cost:.2f}")
                
                result["reason"] = "; ".join(reasons)
            
            logger.info(f"Dry run completed: {result}")
            return result
            
        except Exception as e:
            logger.error(f"Dry run failed: {e}")
            return {"success": False, "reason": str(e)}
    
    async def _create_full_processing_job(self, processing_options: Dict[str, Any]) -> BatchJob:
        """전량 처리 작업 생성"""
        
        job_id = f"full_processing_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # 총 케이스 수 확인
        total_cases = await self._count_total_cases()
        
        batch_job = BatchJob(
            job_id=job_id,
            mode=ProcessingMode.FULL_PROCESSING,
            sample_size=total_cases,
            stratification_criteria={},  # 전량이므로 층화 없음
            rules_version=await self._get_current_rules_version(),
            total_cases=total_cases,
            status=ProcessingStatus.PENDING
        )
        
        # 처리 옵션 저장
        batch_job.processing_options = processing_options
        
        return batch_job
    
    async def _execute_full_processing(self, batch_job: BatchJob) -> Dict[str, Any]:
        """전량 처리 실행"""
        
        try:
            logger.info(f"Starting full processing job: {batch_job.job_id}")
            
            # 처리 통계 초기화
            self.processing_stats.update({
                "total_cases": batch_job.total_cases,
                "processed_cases": 0,
                "failed_cases": 0,
                "start_time": datetime.now(),
                "current_batch": 0
            })
            
            # 배치 설정
            batch_size = batch_job.processing_options.get("batch_size", 1000)
            max_concurrent = batch_job.processing_options.get("max_concurrent", 10)
            
            total_batches = math.ceil(batch_job.total_cases / batch_size)
            self.processing_stats["total_batches"] = total_batches
            
            # 예상 완료 시간 계산
            estimated_duration_hours = await self._estimate_processing_duration(
                batch_job.total_cases, batch_size, max_concurrent
            )
            estimated_completion = datetime.now() + timedelta(hours=estimated_duration_hours)
            self.processing_stats["estimated_completion"] = estimated_completion
            
            # 비용 추정
            estimated_cost = await self._estimate_full_processing_cost()
            
            # 작업 상태 업데이트
            batch_job.status = ProcessingStatus.IN_PROGRESS
            batch_job.start_time = datetime.now()
            batch_job.estimated_cost = estimated_cost
            
            # 백그라운드에서 처리 시작
            asyncio.create_task(self._process_all_batches(batch_job, batch_size, max_concurrent))
            
            return {
                "estimated_duration_hours": estimated_duration_hours,
                "estimated_completion": estimated_completion,
                "estimated_cost": estimated_cost,
                "total_batches": total_batches,
                "batch_size": batch_size
            }
            
        except Exception as e:
            logger.error(f"Failed to execute full processing: {e}")
            raise
    
    async def _process_all_batches(
        self, 
        batch_job: BatchJob, 
        batch_size: int, 
        max_concurrent: int
    ):
        """모든 배치 처리 (백그라운드 실행)"""
        
        try:
            offset = 0
            batch_number = 0
            
            while offset < batch_job.total_cases:
                batch_number += 1
                self.processing_stats["current_batch"] = batch_number
                
                logger.info(f"Processing batch {batch_number}/{self.processing_stats['total_batches']}")
                
                # 배치 데이터 가져오기
                batch_cases = await self._get_batch_cases(offset, batch_size)
                
                if not batch_cases:
                    break
                
                # 배치 처리
                batch_results = await self._process_batch_with_concurrency(
                    batch_cases, max_concurrent
                )
                
                # 결과 저장 및 통계 업데이트
                await self._save_batch_results(batch_results)
                self._update_processing_stats(batch_results)
                
                # 진행률 업데이트
                progress = min(100, (self.processing_stats["processed_cases"] + self.processing_stats["failed_cases"]) / batch_job.total_cases * 100)
                batch_job.progress = int(progress)
                
                # 다음 배치로
                offset += batch_size
                
                # 중단 요청 확인
                if await self._check_stop_requested():
                    logger.info("Stop requested, pausing processing")
                    batch_job.status = ProcessingStatus.CANCELLED
                    break
            
            # 완료 처리
            if batch_job.status != ProcessingStatus.CANCELLED:
                batch_job.status = ProcessingStatus.COMPLETED
                batch_job.end_time = datetime.now()
                
                # 최종 리포트 생성
                await self._generate_final_report(batch_job)
            
            logger.info(f"Full processing completed: {batch_job.job_id}")
            
        except Exception as e:
            logger.error(f"Full processing failed: {e}")
            batch_job.status = ProcessingStatus.FAILED
    
    async def _process_batch_with_concurrency(
        self, 
        batch_cases: List[Dict[str, Any]], 
        max_concurrent: int
    ) -> List[Dict[str, Any]]:
        """동시성 제어하며 배치 처리"""
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def process_single_case_with_semaphore(case_data):
            async with semaphore:
                return await self._process_single_case_full(case_data)
        
        # 병렬 처리
        tasks = [
            process_single_case_with_semaphore(case_data)
            for case_data in batch_cases
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # 예외 처리
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append({
                    "case_id": batch_cases[i].get("case_id", f"unknown_{i}"),
                    "success": False,
                    "error": str(result)
                })
            else:
                processed_results.append(result)
        
        return processed_results
    
    async def _process_single_case_full(self, case_data: Dict[str, Any]) -> Dict[str, Any]:
        """단일 케이스 전량 처리"""
        
        try:
            start_time = datetime.now()
            
            # 메타데이터 준비
            metadata = {
                "court_type": case_data.get("court_type"),
                "case_type": case_data.get("case_type"),
                "year": case_data.get("year"),
                "format_type": case_data.get("format_type")
            }
            
            # DSL 규칙 적용
            original_content = case_data.get("content", "")
            processed_content, rule_results = dsl_manager.apply_rules(original_content)
            applied_rules = [result['rule_id'] for result in rule_results['applied_rules']]
            
            end_time = datetime.now()
            processing_time_ms = int((end_time - start_time).total_seconds() * 1000)
            
            # 토큰 수 계산
            token_count_before = self.openai_service.calculate_token_count(original_content)
            token_count_after = self.openai_service.calculate_token_count(processed_content)
            
            # cases 컬렉션에 전량 처리 결과 저장
            try:
                from app.core.database import db_manager
                cases_collection = db_manager.get_collection("cases")
                
                if cases_collection is not None:
                    case_result = {
                        "original_id": str(case_data["_id"]),
                        "precedent_id": case_data.get("precedent_id", ""),
                        "case_name": case_data.get("case_name", ""),
                        "case_number": case_data.get("case_number", ""),
                        "court_name": case_data.get("court_name", ""),
                        "court_type": case_data.get("court_type", ""),
                        "decision_date": case_data.get("decision_date", ""),
                        "original_content": original_content,
                        "processed_content": processed_content,
                        "rules_version": self._get_current_rules_version(),
                        "processing_mode": "full",
                        "processing_time_ms": processing_time_ms,
                        "token_count_before": int(token_count_before),
                        "token_count_after": int(token_count_after),
                        "token_reduction_percent": ((int(token_count_before) - int(token_count_after)) / int(token_count_before) * 100) if int(token_count_before) > 0 else 0,
                        "applied_rules": applied_rules,
                        "status": "completed",
                        "created_at": datetime.now().isoformat(),
                        "updated_at": datetime.now().isoformat()
                    }
                    
                    await cases_collection.update_one(
                        {"original_id": str(case_data["_id"])},
                        {"$set": case_result},
                        upsert=True
                    )
                    
                    logger.info(f"Saved full processing result for case {case_data.get('_id')}")
            except Exception as save_error:
                logger.error(f"Failed to save full processing result: {save_error}")
            
            return {
                "case_id": str(case_data["_id"]),
                "success": True,
                "before_content": original_content,
                "after_content": processed_content,
                "applied_rules": applied_rules,
                "processing_time_ms": processing_time_ms,
                "token_count_before": int(token_count_before),
                "token_count_after": int(token_count_after),
                "metadata": metadata
            }
            
        except Exception as e:
            logger.error(f"Failed to process case {case_data.get('case_id')}: {e}")
            return {
                "case_id": case_data.get("case_id", "unknown"),
                "success": False,
                "error": str(e)
            }
    
    async def _save_batch_results(self, batch_results: List[Dict[str, Any]]):
        """배치 결과 저장 - 이미 개별 케이스 처리에서 cases 컬렉션에 저장했으므로 통계만 업데이트"""
        
        # 통계 목적으로만 사용 - 실제 저장은 _process_single_case_full에서 이미 완료
        success_count = sum(1 for result in batch_results if result.get("success", False))
        failure_count = len(batch_results) - success_count
        
        logger.info(f"Batch results: {success_count} successes, {failure_count} failures")
        
        # 필요시 processing_results 컬렉션에 메타데이터 저장
        try:
            for result in batch_results:
                if result["success"]:
                    processing_result = ProcessingResult(
                        case_id=result["case_id"],
                        rules_version=await self._get_current_rules_version(),
                        metrics=QualityMetrics(nrr=0, fpr=0, ss=0, token_reduction=0),  # 전량 처리에서는 평가 생략
                        before_content=result["before_content"],
                        after_content=result["after_content"],
                        diff_summary="",
                        processing_time_ms=result["processing_time_ms"],
                        token_count_before=result["token_count_before"],
                        token_count_after=result["token_count_after"],
                        errors=[],
                        warnings=[]
                    )
                    
                    await result_repo.save_result(processing_result.dict())
                    
        except Exception as e:
            logger.error(f"Failed to save processing results metadata: {e}")
    
    def _update_processing_stats(self, batch_results: List[Dict[str, Any]]):
        """처리 통계 업데이트"""
        
        for result in batch_results:
            if result["success"]:
                self.processing_stats["processed_cases"] += 1
            else:
                self.processing_stats["failed_cases"] += 1
    
    async def _check_stop_requested(self) -> bool:
        """중단 요청 확인"""
        # 실제로는 Redis나 데이터베이스에서 중단 플래그 확인
        return False
    
    async def stop_processing(self, job_id: str) -> Dict[str, Any]:
        """처리 중단"""
        
        if not self.current_job or self.current_job.job_id != job_id:
            return {"success": False, "reason": "Job not found or not active"}
        
        # 중단 플래그 설정
        # 실제로는 Redis나 데이터베이스에 플래그 저장
        
        return {
            "success": True,
            "message": "Stop request submitted. Processing will pause after current batch.",
            "current_progress": self.processing_stats
        }
    
    async def resume_processing(self, job_id: str) -> Dict[str, Any]:
        """처리 재개"""
        
        if not self.current_job or self.current_job.job_id != job_id:
            return {"success": False, "reason": "Job not found"}
        
        if self.current_job.status != ProcessingStatus.CANCELLED:
            return {"success": False, "reason": "Job is not in paused state"}
        
        # 재개 로직
        self.current_job.status = ProcessingStatus.IN_PROGRESS
        
        # 백그라운드에서 처리 재시작
        asyncio.create_task(self._resume_processing_from_checkpoint(self.current_job))
        
        return {
            "success": True,
            "message": "Processing resumed",
            "current_progress": self.processing_stats
        }
    
    async def get_processing_status(self, job_id: str) -> Dict[str, Any]:
        """처리 상태 조회"""
        
        if not self.current_job or self.current_job.job_id != job_id:
            return {"error": "Job not found"}
        
        # 현재 진행률 계산
        total_processed = self.processing_stats["processed_cases"] + self.processing_stats["failed_cases"]
        progress_percentage = (total_processed / self.processing_stats["total_cases"]) * 100 if self.processing_stats["total_cases"] > 0 else 0
        
        # 남은 시간 추정
        if self.processing_stats["start_time"] and total_processed > 0:
            elapsed_time = datetime.now() - self.processing_stats["start_time"]
            avg_time_per_case = elapsed_time.total_seconds() / total_processed
            remaining_cases = self.processing_stats["total_cases"] - total_processed
            estimated_remaining_seconds = remaining_cases * avg_time_per_case
            estimated_completion = datetime.now() + timedelta(seconds=estimated_remaining_seconds)
        else:
            estimated_completion = self.processing_stats.get("estimated_completion")
        
        return {
            "job_id": job_id,
            "status": self.current_job.status.value,
            "progress_percentage": round(progress_percentage, 2),
            "processed_cases": self.processing_stats["processed_cases"],
            "failed_cases": self.processing_stats["failed_cases"],
            "total_cases": self.processing_stats["total_cases"],
            "current_batch": self.processing_stats["current_batch"],
            "total_batches": self.processing_stats["total_batches"],
            "start_time": self.processing_stats["start_time"],
            "estimated_completion": estimated_completion,
            "success_rate": (self.processing_stats["processed_cases"] / total_processed * 100) if total_processed > 0 else 0
        }
    
    # Helper methods
    async def _get_latest_batch_results(self) -> Optional[Dict[str, Any]]:
        """최신 배치 결과 가져오기"""
        # 실제로는 데이터베이스에서 조회
        return {"quality_gates_passed": True}
    
    def _check_quality_gates_passed(self, batch_results: Dict[str, Any]) -> bool:
        """품질 게이트 통과 여부 확인"""
        return batch_results.get("quality_gates_passed", False)
    
    async def _check_recent_regressions(self) -> Dict[str, Any]:
        """최근 회귀 테스트 확인"""
        # 실제로는 데이터베이스에서 조회
        return {"passed": True, "count": 0}
    
    async def _check_rules_stability(self) -> Dict[str, Any]:
        """규칙 안정성 확인"""
        # 실제로는 규칙 변경 이력 확인
        return {"stable": True, "reason": "No recent changes"}
    
    async def _process_batch_parallel(self, batch_cases: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """배치 병렬 처리"""
        return [{"success": True} for _ in batch_cases]  # 간단한 구현
    
    async def _estimate_full_processing_cost(self, avg_time_per_case: float = 5.0) -> float:
        """전량 처리 비용 추정"""
        # 간단한 비용 계산
        total_cases = 160000
        estimated_tokens_per_case = 2000
        total_tokens = total_cases * estimated_tokens_per_case
        
        # GPT-4 Turbo 가격 기준
        cost_per_1k_tokens = 0.01
        total_cost = (total_tokens / 1000) * cost_per_1k_tokens
        
        return total_cost
    
    async def _count_total_cases(self) -> int:
        """총 케이스 수 확인"""
        # 실제로는 데이터베이스 쿼리
        return 160000
    
    async def _get_current_rules_version(self) -> str:
        """현재 규칙 버전 가져오기"""
        return "v1.0.0"
    
    async def _estimate_processing_duration(
        self, 
        total_cases: int, 
        batch_size: int, 
        max_concurrent: int
    ) -> float:
        """처리 시간 추정"""
        # 간단한 추정 (실제로는 더 정교한 계산)
        avg_time_per_case = 5.0  # 5초
        total_time_seconds = total_cases * avg_time_per_case / max_concurrent
        return total_time_seconds / 3600  # 시간 단위로 변환
    
    async def _get_batch_cases(self, offset: int, batch_size: int) -> List[Dict[str, Any]]:
        """배치 케이스 가져오기 (precedents_v2에서)"""
        try:
            from app.core.database import db_manager
            collection = db_manager.get_collection("precedents_v2")
            
            if collection is None:
                logger.error("Database connection unavailable")
                return []
            
            # 페이징으로 케이스 조회
            cursor = collection.find({}).skip(offset).limit(batch_size)
            cases = await cursor.to_list(length=batch_size)
            
            logger.info(f"Retrieved {len(cases)} cases from offset {offset}")
            return cases
            
        except Exception as e:
            logger.error(f"Failed to get batch cases: {e}")
            return []
    
    async def _generate_final_report(self, batch_job: BatchJob):
        """최종 리포트 생성"""
        logger.info(f"Generating final report for job: {batch_job.job_id}")
        # 실제로는 상세한 리포트 생성 및 저장
    
    async def _resume_processing_from_checkpoint(self, batch_job: BatchJob):
        """체크포인트에서 처리 재개"""
        logger.info(f"Resuming processing from checkpoint: {batch_job.job_id}")
        # 실제로는 중단된 지점부터 재시작
    
    def _get_current_rules_version(self) -> str:
        """현재 DSL 규칙 버전 가져오기"""
        try:
            from app.services.dsl_rules import dsl_manager
            return dsl_manager.version
        except Exception as e:
            logger.warning(f"규칙 버전 로드 실패: {e}")
            return "v1.0.0"
