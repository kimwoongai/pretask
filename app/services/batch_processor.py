"""
ë°°ì¹˜ ì²˜ë¦¬ ì„œë¹„ìŠ¤
"""
import asyncio
import uuid
from datetime import datetime
from typing import Dict, List, Any, Optional
import logging
from app.core.database import db_manager
from app.services.openai_service import OpenAIService
from app.services.dsl_rules import dsl_manager
from app.services.auto_patch_engine import auto_patch_engine

logger = logging.getLogger(__name__)


class BatchJob:
    """ë°°ì¹˜ ì‘ì—… í´ë˜ìŠ¤"""
    
    def __init__(self, job_id: str, settings: Dict[str, Any]):
        self.job_id = job_id
        self.settings = settings
        self.status = "pending"
        self.created_at = datetime.now()
        self.started_at = None
        self.completed_at = None
        self.total_cases = 0
        self.processed_cases = 0
        self.success_rate = 0.0
        self.errors = []
        self.results = []
        
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "job_id": self.job_id,
            "status": self.status,
            "settings": self.settings,
            "created_at": self.created_at.isoformat(),
            "started_at": self.started_at.isoformat() if self.started_at else None,
            "completed_at": self.completed_at.isoformat() if self.completed_at else None,
            "total_cases": self.total_cases,
            "processed_cases": self.processed_cases,
            "success_rate": self.success_rate,
            "errors": self.errors,
            "results_count": len(self.results)
        }


class BatchProcessor:
    """ë°°ì¹˜ ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.openai_service = OpenAIService()
        self.active_jobs: Dict[str, BatchJob] = {}
        self.job_history: List[BatchJob] = []
        
    async def start_batch_job(self, settings: Dict[str, Any]) -> str:
        """ë°°ì¹˜ ì‘ì—… ì‹œì‘"""
        job_id = f"batch_{uuid.uuid4().hex[:8]}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        job = BatchJob(job_id, settings)
        self.active_jobs[job_id] = job
        
        logger.info(f"ë°°ì¹˜ ì‘ì—… ì‹œì‘: {job_id}, ì„¤ì •: {settings}")
        print(f"ğŸš€ DEBUG: ë°°ì¹˜ ì‘ì—… ì‹œì‘ - ID: {job_id}")
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
        asyncio.create_task(self._process_batch_job(job))
        
        return job_id
        
    async def _process_batch_job(self, job: BatchJob):
        """ë°°ì¹˜ ì‘ì—… ì²˜ë¦¬"""
        try:
            job.status = "running"
            job.started_at = datetime.now()
            
            print(f"ğŸ”„ DEBUG: ë°°ì¹˜ ì‘ì—… ì‹¤í–‰ ì‹œì‘ - {job.job_id}")
            
            # 1. ìƒ˜í”Œ ì„ ì •
            await self._update_job_status(job, "sampling", "ìƒ˜í”Œ ì„ ì • ì¤‘...")
            sample_cases = await self._select_sample_cases(job.settings)
            job.total_cases = len(sample_cases)
            
            print(f"ğŸ“Š DEBUG: ìƒ˜í”Œ ì„ ì • ì™„ë£Œ - {job.total_cases}ê°œ ì¼€ì´ìŠ¤")
            
            # 2. ë°°ì¹˜ ì „ì²˜ë¦¬ ë° í‰ê°€
            await self._update_job_status(job, "processing", "ë°°ì¹˜ ì „ì²˜ë¦¬ ë° í‰ê°€ ì¤‘...")
            
            if job.settings.get('use_batch_api', True):
                # OpenAI Batch API ì‚¬ìš©
                print(f"ğŸ¤– DEBUG: OpenAI Batch API ì‚¬ìš©")
                batch_results = await self.openai_service.evaluate_batch_cases(sample_cases)
            else:
                # ìˆœì°¨ ì²˜ë¦¬
                print(f"ğŸ”„ DEBUG: ìˆœì°¨ ì²˜ë¦¬ ì‚¬ìš©")
                batch_results = await self._process_sequential(sample_cases, job)
            
            job.results = batch_results
            job.processed_cases = len(batch_results)
            
            print(f"âœ… DEBUG: ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ - {job.processed_cases}ê°œ ì²˜ë¦¬ë¨")
            
            # 3. ê²°ê³¼ ë¶„ì„ ë° íŒ¨ì¹˜ ì ìš©
            await self._update_job_status(job, "analyzing", "ê²°ê³¼ ë¶„ì„ ë° íŒ¨ì¹˜ ì ìš© ì¤‘...")
            await self._analyze_and_apply_patches(job, batch_results)
            
            # 4. ì™„ë£Œ ì²˜ë¦¬
            job.status = "completed"
            job.completed_at = datetime.now()
            job.success_rate = job.processed_cases / job.total_cases if job.total_cases > 0 else 0
            
            print(f"ğŸ‰ DEBUG: ë°°ì¹˜ ì‘ì—… ì™„ë£Œ - {job.job_id}")
            logger.info(f"ë°°ì¹˜ ì‘ì—… ì™„ë£Œ: {job.job_id}, ì„±ê³µë¥ : {job.success_rate:.2%}")
            
        except Exception as e:
            job.status = "failed"
            job.errors.append(str(e))
            print(f"âŒ DEBUG: ë°°ì¹˜ ì‘ì—… ì‹¤íŒ¨ - {job.job_id}: {e}")
            logger.error(f"ë°°ì¹˜ ì‘ì—… ì‹¤íŒ¨: {job.job_id}: {e}")
        finally:
            # í™œì„± ì‘ì—…ì—ì„œ ì œê±°í•˜ê³  íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            if job.job_id in self.active_jobs:
                del self.active_jobs[job.job_id]
            self.job_history.append(job)
            
    async def _select_sample_cases(self, settings: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ìƒ˜í”Œ ì¼€ì´ìŠ¤ ì„ ì •"""
        sample_size = settings.get('sample_size', 10)
        
        print(f"ğŸ“‹ DEBUG: ìƒ˜í”Œ ì„ ì • ì‹œì‘ - í¬ê¸°: {sample_size}")
        
        try:
            # MongoDBì—ì„œ ì¼€ì´ìŠ¤ ì¡°íšŒ
            print(f"ğŸ” DEBUG: MongoDB ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸° ì‹œë„...")
            print(f"ğŸ” DEBUG: db_manager ê°ì²´: {type(db_manager)}")
            print(f"ğŸ” DEBUG: db_manager ìƒíƒœ: {hasattr(db_manager, 'get_collection')}")
            
            collection = db_manager.get_collection('cases')
            print(f"ğŸ” DEBUG: ì»¬ë ‰ì…˜ ê°ì²´: {type(collection)}")
            print(f"ğŸ” DEBUG: ì»¬ë ‰ì…˜ None ì—¬ë¶€: {collection is None}")
            
            if collection is None:
                raise Exception("cases ì»¬ë ‰ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            print(f"âœ… DEBUG: cases ì»¬ë ‰ì…˜ ì—°ê²° ì„±ê³µ")
            
            # ì¸µí™” ìƒ˜í”Œë§ (ê°„ë‹¨í•œ ë²„ì „)
            pipeline = [
                {"$match": {"content": {"$exists": True, "$ne": ""}}},
                {"$sample": {"size": sample_size}}
            ]
            
            print(f"ğŸ” DEBUG: ì§‘ê³„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘...")
            cursor = collection.aggregate(pipeline)
            cases = await cursor.to_list(length=sample_size)
            
            print(f"âœ… DEBUG: MongoDBì—ì„œ {len(cases)}ê°œ ì¼€ì´ìŠ¤ ì¡°íšŒ ì™„ë£Œ")
            
            # ì¼€ì´ìŠ¤ ë°ì´í„° ë³€í™˜
            sample_cases = []
            for case in cases:
                case_data = {
                    "case_id": str(case.get("_id")),
                    "before_content": case.get("content", ""),
                    "after_content": "",  # ì „ì²˜ë¦¬ í›„ ì±„ì›Œì§
                    "metadata": {
                        "court_type": case.get("court_type", ""),
                        "case_type": case.get("case_type", ""),
                        "year": case.get("year", "")
                    }
                }
                
                # DSL ê·œì¹™ ì ìš©í•˜ì—¬ ì „ì²˜ë¦¬
                processed_content, rule_results = dsl_manager.apply_rules(
                    case_data["before_content"], 
                    rule_types=None
                )
                case_data["after_content"] = processed_content
                
                sample_cases.append(case_data)
            
            print(f"âœ… DEBUG: ìƒ˜í”Œ ì„ ì • ì™„ë£Œ - {len(sample_cases)}ê°œ ì¼€ì´ìŠ¤")
            return sample_cases
            
        except Exception as e:
            print(f"âŒ DEBUG: ìƒ˜í”Œ ì„ ì • ì‹¤íŒ¨: {e}")
            print(f"âŒ DEBUG: ì˜¤ë¥˜ íƒ€ì…: {type(e)}")
            print(f"âŒ DEBUG: ì˜¤ë¥˜ ìƒì„¸: {str(e)}")
            logger.error(f"ìƒ˜í”Œ ì„ ì • ì‹¤íŒ¨: {e}")
            raise
    
    async def _process_sequential(self, cases: List[Dict[str, Any]], job: BatchJob) -> List[Any]:
        """ìˆœì°¨ ì²˜ë¦¬ (Batch API ëŒ€ì‹ )"""
        results = []
        
        for i, case in enumerate(cases):
            try:
                print(f"ğŸ”„ DEBUG: ìˆœì°¨ ì²˜ë¦¬ {i+1}/{len(cases)} - {case['case_id']}")
                
                # ë‹¨ì¼ ì¼€ì´ìŠ¤ í‰ê°€
                metrics, errors, suggestions_json = await self.openai_service.evaluate_single_case(
                    case["before_content"],
                    case["after_content"], 
                    case["metadata"]
                )
                
                results.append((case["case_id"], metrics, errors, suggestions_json))
                job.processed_cases = i + 1
                
            except Exception as e:
                print(f"âŒ DEBUG: ìˆœì°¨ ì²˜ë¦¬ ì‹¤íŒ¨ - {case['case_id']}: {e}")
                job.errors.append(f"ì¼€ì´ìŠ¤ {case['case_id']} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        
        return results
    
    async def _analyze_and_apply_patches(self, job: BatchJob, results: List[Any]):
        """ê²°ê³¼ ë¶„ì„ ë° íŒ¨ì¹˜ ì ìš©"""
        print(f"ğŸ” DEBUG: ê²°ê³¼ ë¶„ì„ ë° íŒ¨ì¹˜ ì ìš© ì‹œì‘")
        
        try:
            all_suggestions = []
            
            # ëª¨ë“  ê²°ê³¼ì—ì„œ ì œì•ˆ ìˆ˜ì§‘
            for result in results:
                case_id, metrics, errors, suggestions_json = result
                
                if suggestions_json:
                    try:
                        import json
                        suggestions_data = json.loads(suggestions_json)
                        suggestions = suggestions_data.get("suggestions", [])
                        
                        for suggestion in suggestions:
                            # PatchSuggestion ê°ì²´ë¡œ ë³€í™˜
                            from app.services.auto_patch_engine import PatchSuggestion
                            patch = PatchSuggestion(
                                suggestion_id=f"{case_id}_{len(all_suggestions)}",
                                rule_type=suggestion.get("rule_type", "noise_removal"),
                                description=suggestion.get("description", ""),
                                pattern_before=suggestion.get("pattern_before", ""),
                                pattern_after=suggestion.get("pattern_after", ""),
                                confidence_score=suggestion.get("confidence_score", 0.8),
                                estimated_improvement=suggestion.get("estimated_improvement", ""),
                                applicable_cases=suggestion.get("applicable_cases", [])
                            )
                            all_suggestions.append(patch)
                            
                    except Exception as e:
                        print(f"âš ï¸ DEBUG: ì œì•ˆ íŒŒì‹± ì‹¤íŒ¨ - {case_id}: {e}")
            
            print(f"ğŸ“ˆ DEBUG: ì´ {len(all_suggestions)}ê°œ ì œì•ˆ ìˆ˜ì§‘ë¨")
            
            # ìë™ íŒ¨ì¹˜ ì ìš©
            if all_suggestions:
                patch_results = auto_patch_engine.auto_apply_patches(
                    all_suggestions,
                    auto_apply_threshold=0.5  # ëª¨ë“  ì œì•ˆ ì ìš©
                )
                
                print(f"ğŸ”§ DEBUG: íŒ¨ì¹˜ ì ìš© ê²°ê³¼ - ìë™ ì ìš©: {patch_results['auto_applied']}ê°œ")
                
        except Exception as e:
            print(f"âŒ DEBUG: ê²°ê³¼ ë¶„ì„ ì‹¤íŒ¨: {e}")
            logger.error(f"ê²°ê³¼ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    async def _update_job_status(self, job: BatchJob, status: str, message: str):
        """ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸"""
        job.status = status
        print(f"ğŸ“Š DEBUG: ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸ - {job.job_id}: {status} - {message}")
        logger.info(f"ë°°ì¹˜ ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸: {job.job_id} - {status}")
        
    def stop_batch_job(self, job_id: str) -> bool:
        """ë°°ì¹˜ ì‘ì—… ì¤‘ì§€"""
        if job_id in self.active_jobs:
            job = self.active_jobs[job_id]
            job.status = "cancelled"
            job.completed_at = datetime.now()
            
            # íˆìŠ¤í† ë¦¬ë¡œ ì´ë™
            del self.active_jobs[job_id]
            self.job_history.append(job)
            
            print(f"â¹ï¸ DEBUG: ë°°ì¹˜ ì‘ì—… ì¤‘ì§€ - {job_id}")
            logger.info(f"ë°°ì¹˜ ì‘ì—… ì¤‘ì§€: {job_id}")
            return True
        return False
    
    def get_job_status(self, job_id: str) -> Optional[Dict[str, Any]]:
        """ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
        # í™œì„± ì‘ì—…ì—ì„œ ì°¾ê¸°
        if job_id in self.active_jobs:
            return self.active_jobs[job_id].to_dict()
        
        # íˆìŠ¤í† ë¦¬ì—ì„œ ì°¾ê¸°
        for job in self.job_history:
            if job.job_id == job_id:
                return job.to_dict()
        
        return None
    
    def get_batch_stats(self) -> Dict[str, Any]:
        """ë°°ì¹˜ í†µê³„ ì¡°íšŒ"""
        active_count = len(self.active_jobs)
        total_jobs = len(self.job_history) + active_count
        
        # ìµœê·¼ ì™„ë£Œëœ ì‘ì—…ë“¤ì˜ ì„±ê³µë¥  ê³„ì‚°
        completed_jobs = [job for job in self.job_history if job.status == "completed"]
        avg_success_rate = 0.0
        if completed_jobs:
            avg_success_rate = sum(job.success_rate for job in completed_jobs) / len(completed_jobs)
        
        return {
            "active_jobs": active_count,
            "total_jobs": total_jobs,
            "completed_jobs": len(completed_jobs),
            "avg_success_rate": avg_success_rate,
            "status": "running" if active_count > 0 else "idle"
        }
    
    def get_job_history(self, limit: int = 10) -> List[Dict[str, Any]]:
        """ì‘ì—… íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        # ìµœì‹  ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_history = sorted(self.job_history, key=lambda x: x.created_at, reverse=True)
        return [job.to_dict() for job in sorted_history[:limit]]


# ì „ì—­ ë°°ì¹˜ í”„ë¡œì„¸ì„œ ì¸ìŠ¤í„´ìŠ¤
batch_processor = BatchProcessor()