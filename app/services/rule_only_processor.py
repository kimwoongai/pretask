"""
ê·œì¹™ ì „ìš© ì²˜ë¦¬ê¸° - AI í‰ê°€ ì—†ì´ ê¸°ë³¸ ê·œì¹™ë§Œìœ¼ë¡œ ì „ì²˜ë¦¬
"""
import asyncio
from typing import Dict, List, Any, Optional
from datetime import datetime
import logging

from app.services.dsl_rules import dsl_manager
from app.core.database import db_manager

logger = logging.getLogger(__name__)


class RuleOnlyProcessor:
    """ê·œì¹™ ì „ìš© ì²˜ë¦¬ê¸° - AI í‰ê°€ ì—†ì´ ê¸°ë³¸ ê·œì¹™ë§Œ ì‚¬ìš©"""
    
    def __init__(self):
        self.processed_count = 0
        self.error_count = 0
        self.start_time = None
        
    async def process_all_precedents(self, batch_size: int = 100) -> Dict[str, Any]:
        """ëª¨ë“  íŒë¡€ë¥¼ ê¸°ë³¸ ê·œì¹™ë§Œìœ¼ë¡œ ì „ì²˜ë¦¬"""
        try:
            self.start_time = datetime.now()
            print(f"ğŸš€ ê¸°ë³¸ ê·œì¹™ ì „ìš© ì „ì²˜ë¦¬ ì‹œì‘ - ë°°ì¹˜ í¬ê¸°: {batch_size}")
            
            # MongoDB ì»¬ë ‰ì…˜ ì—°ê²°
            source_collection = db_manager.get_collection('precedents_v2')
            target_collection = db_manager.get_collection('processed_cases')
            
            if not source_collection:
                raise Exception("precedents_v2 ì»¬ë ‰ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # ì „ì²´ ë¬¸ì„œ ìˆ˜ í™•ì¸
            total_count = await source_collection.count_documents({})
            print(f"ğŸ“Š ì „ì²´ íŒë¡€ ìˆ˜: {total_count:,}ê°œ")
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            processed = 0
            skip = 0
            
            while processed < total_count:
                print(f"ğŸ“‹ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {processed:,}/{total_count:,} ({processed/total_count*100:.1f}%)")
                
                # ë°°ì¹˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                cursor = source_collection.find({}).skip(skip).limit(batch_size)
                batch_docs = await cursor.to_list(length=batch_size)
                
                if not batch_docs:
                    break
                
                # ë°°ì¹˜ ì²˜ë¦¬
                batch_results = await self._process_batch(batch_docs)
                
                # ê²°ê³¼ ì €ì¥
                if batch_results:
                    await target_collection.insert_many(batch_results)
                    print(f"âœ… ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: {len(batch_results)}ê°œ")
                
                processed += len(batch_docs)
                skip += batch_size
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                if processed % 1000 == 0:
                    elapsed = (datetime.now() - self.start_time).total_seconds()
                    rate = processed / elapsed if elapsed > 0 else 0
                    remaining = (total_count - processed) / rate if rate > 0 else 0
                    print(f"â±ï¸ ì²˜ë¦¬ ì†ë„: {rate:.1f}ê±´/ì´ˆ, ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {remaining/60:.1f}ë¶„")
            
            # ìµœì¢… ê²°ê³¼
            end_time = datetime.now()
            total_time = (end_time - self.start_time).total_seconds()
            
            return {
                "status": "completed",
                "total_processed": self.processed_count,
                "total_errors": self.error_count,
                "processing_time_seconds": total_time,
                "average_rate": self.processed_count / total_time if total_time > 0 else 0,
                "start_time": self.start_time.isoformat(),
                "end_time": end_time.isoformat()
            }
            
        except Exception as e:
            logger.error(f"ì „ì²´ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "processed_count": self.processed_count,
                "error_count": self.error_count
            }
    
    async def _process_batch(self, batch_docs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ ë¬¸ì„œë“¤ì„ ì²˜ë¦¬"""
        results = []
        
        for doc in batch_docs:
            try:
                result = await self._process_single_document(doc)
                if result:
                    results.append(result)
                    self.processed_count += 1
            except Exception as e:
                self.error_count += 1
                logger.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨ {doc.get('_id', 'unknown')}: {e}")
        
        return results
    
    async def _process_single_document(self, doc: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ë‹¨ì¼ ë¬¸ì„œ ì²˜ë¦¬"""
        try:
            # í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
            original_content = self._extract_content(doc)
            if not original_content or len(original_content) < 50:
                return None
            
            # ê¸°ë³¸ ê·œì¹™ ì ìš©
            processed_content, rule_results = dsl_manager.apply_rules(original_content)
            
            # ì²˜ë¦¬ í†µê³„
            original_length = len(original_content)
            processed_length = len(processed_content)
            reduction_rate = (original_length - processed_length) / original_length * 100 if original_length > 0 else 0
            
            # ê²°ê³¼ êµ¬ì„±
            result = {
                "original_id": str(doc.get("_id", "")),
                "precedent_id": doc.get("precedent_id", ""),
                "case_name": doc.get("case_name", ""),
                "case_number": doc.get("case_number", ""),
                "court_name": doc.get("court_name", ""),
                "court_type": doc.get("court_type", ""),
                "decision_date": doc.get("decision_date", ""),
                "original_content": original_content,
                "processed_content": processed_content,
                "processing_mode": "rule_only",
                "rules_version": dsl_manager.version,
                "original_length": original_length,
                "processed_length": processed_length,
                "reduction_rate": round(reduction_rate, 2),
                "applied_rules": [rule["rule_id"] for rule in rule_results["applied_rules"]],
                "applied_rule_count": rule_results["stats"]["applied_rule_count"],
                "rule_types_used": rule_results["stats"]["rule_types"],
                "processed_at": datetime.now().isoformat(),
                "status": "completed"
            }
            
            return result
            
        except Exception as e:
            logger.error(f"ë‹¨ì¼ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_content(self, doc: Dict[str, Any]) -> Optional[str]:
        """ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ"""
        # ë‹¤ì–‘í•œ í•„ë“œëª… ì‹œë„
        content_fields = ['content', 'text', 'body', 'document_text', 'full_text']
        
        for field in content_fields:
            content = doc.get(field)
            if content and isinstance(content, str) and len(content.strip()) > 0:
                return content.strip()
        
        return None
    
    def get_progress_stats(self) -> Dict[str, Any]:
        """ì§„í–‰ ìƒí™© í†µê³„"""
        if self.start_time:
            elapsed = (datetime.now() - self.start_time).total_seconds()
            rate = self.processed_count / elapsed if elapsed > 0 else 0
        else:
            elapsed = 0
            rate = 0
        
        return {
            "processed_count": self.processed_count,
            "error_count": self.error_count,
            "elapsed_seconds": elapsed,
            "processing_rate": round(rate, 2),
            "status": "running" if self.start_time else "idle"
        }


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
rule_only_processor = RuleOnlyProcessor()
