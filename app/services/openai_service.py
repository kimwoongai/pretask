"""
OpenAI API ì„œë¹„ìŠ¤
"""
import asyncio
import json
from typing import Dict, List, Any, Optional, Tuple
import openai
from app.core.config import settings
from app.models.document import QualityMetrics
import logging

logger = logging.getLogger(__name__)


class OpenAIService:
    """OpenAI API ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.client = openai.AsyncOpenAI(api_key=settings.openai_api_key)
        self.model = settings.openai_model
    
    async def evaluate_single_case(
        self, 
        before_content: str, 
        after_content: str, 
        case_metadata: Dict[str, Any]
    ) -> Tuple[QualityMetrics, List[str], str]:
        """ë‹¨ì¼ ì¼€ì´ìŠ¤ í‰ê°€"""
        
        print("ğŸ” DEBUG: OpenAI evaluate_single_case called")
        prompt = self._create_evaluation_prompt(before_content, after_content, case_metadata)
        print(f"ğŸ” DEBUG: Prompt created, length: {len(prompt)}")
        
        try:
            print("ğŸ” DEBUG: Making OpenAI API call...")
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë²•ë¥  ë¬¸ì„œ ì „ì²˜ë¦¬ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=2000
            )
            
            print("ğŸ” DEBUG: OpenAI API call successful")
            result_text = response.choices[0].message.content
            print(f"ğŸ” DEBUG: OpenAI raw response length: {len(result_text) if result_text else 0}")
            print(f"ğŸ” DEBUG: OpenAI raw response: {result_text}")
            logger.info(f"OpenAI raw response: {result_text}")
            return self._parse_evaluation_result(result_text, before_content, after_content)
            
        except Exception as e:
            logger.error(f"Failed to evaluate case: {e}")
            logger.error(f"OpenAI API key (first 20 chars): {settings.openai_api_key[:20] if settings.openai_api_key else 'None'}")
            logger.error(f"OpenAI model: {self.model}")
            import traceback
            logger.error(f"Full traceback: {traceback.format_exc()}")
            raise
    
    async def evaluate_batch_cases(
        self, 
        cases: List[Dict[str, Any]]
    ) -> List[Tuple[str, QualityMetrics, List[str], str]]:
        """ë°°ì¹˜ ì¼€ì´ìŠ¤ í‰ê°€"""
        
        batch_requests = []
        
        for i, case in enumerate(cases):
            prompt = self._create_evaluation_prompt(
                case["before_content"], 
                case["after_content"], 
                case.get("metadata", {})
            )
            
            batch_requests.append({
                "custom_id": f"eval_{case['case_id']}_{i}",
                "method": "POST",
                "url": "/v1/chat/completions",
                "body": {
                    "model": self.model,
                    "messages": [
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ë²•ë¥  ë¬¸ì„œ ì „ì²˜ë¦¬ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.1,
                    "max_tokens": 2000
                }
            })
        
        try:
            # Batch API ìš”ì²­ ìƒì„±
            batch_file = await self._create_batch_file(batch_requests)
            
            # Batch ì‘ì—… ì‹œì‘
            batch = await self.client.batches.create(
                input_file_id=batch_file.id,
                endpoint="/v1/chat/completions",
                completion_window="24h"
            )
            
            # ì™„ë£Œ ëŒ€ê¸°
            batch_result = await self._wait_for_batch_completion(batch.id)
            
            # ê²°ê³¼ íŒŒì‹±
            return await self._parse_batch_results(batch_result, cases)
            
        except Exception as e:
            logger.error(f"Failed to evaluate batch cases: {e}")
            raise
    
    async def generate_improvement_suggestions(
        self, 
        failure_patterns: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        
        suggestions = []
        
        for pattern in failure_patterns:
            prompt = self._create_improvement_prompt(pattern)
            
            try:
                response = await self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ë¬¸ì„œ ì „ì²˜ë¦¬ ê·œì¹™ ê°œì„  ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3,
                    max_tokens=1000
                )
                
                suggestion_text = response.choices[0].message.content
                suggestion = self._parse_improvement_suggestion(suggestion_text, pattern)
                
                if suggestion:
                    suggestions.append(suggestion)
                    
            except Exception as e:
                logger.warning(f"Failed to generate suggestion for pattern: {e}")
                continue
        
        return suggestions
    
    def _create_evaluation_prompt(
        self, 
        before_content: str, 
        after_content: str, 
        metadata: Dict[str, Any]
    ) -> str:
        """í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        return f"""
ë‹¤ìŒ ë²•ë¥  ë¬¸ì„œì˜ ì „ì²˜ë¦¬ ê²°ê³¼ë¥¼ í‰ê°€í•˜ê³  êµ¬ì²´ì ì¸ ê°œì„  ì œì•ˆì„ ì œê³µí•´ì£¼ì„¸ìš”.

**ë¬¸ì„œ ì •ë³´:**
- ë²•ì› ìœ í˜•: {metadata.get('court_type', 'N/A')}
- ì‚¬ê±´ ìœ í˜•: {metadata.get('case_type', 'N/A')}
- ì—°ë„: {metadata.get('year', 'N/A')}

**ì „ì²˜ë¦¬ ì „ ë‚´ìš© (ì²˜ìŒ 800ì):**
{before_content[:800]}...

**ì „ì²˜ë¦¬ í›„ ë‚´ìš© (ì²˜ìŒ 800ì):**
{after_content[:800]}...

**í‰ê°€ ì‘ì—…:**
1. ì „ì²˜ë¦¬ í’ˆì§ˆì„ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”
2. ë°œê²¬ëœ ë¬¸ì œì ë“¤ì„ errors ë°°ì—´ì— ë‚˜ì—´í•˜ì„¸ìš”
3. **ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ê°œì„  ê°€ëŠ¥í•œ íŒ¨í„´ì„ ì°¾ì•„ ì œì•ˆí•˜ì„¸ìš”**

**ê°œì„  ì œì•ˆ ìƒì„± (ë³´ìˆ˜ì ì´ê³  ì•ˆì „í•œ ë…¸ì´ì¦ˆ ì œê±°):**
- **í™•ì‹¤í•œ ë…¸ì´ì¦ˆë§Œ ì œê±°í•˜ëŠ” êµ¬ì²´ì ì´ê³  ì•ˆì „í•œ íŒ¨í„´ì„ ì œì•ˆí•˜ì„¸ìš”**
- **ëª©í‘œ: ì‚¬ì‹¤ê´€ê³„ëŠ” ì ˆëŒ€ ê±´ë“œë¦¬ì§€ ì•Šê³  ëª…í™•í•œ ë…¸ì´ì¦ˆë§Œ ì œê±°**
- **ì¤‘ìš”: ê´‘ë²”ìœ„í•œ ì •ê·œì‹(.*?) ì‚¬ìš©ì„ í”¼í•˜ê³  ë¼ì¸ ë‹¨ìœ„ ë§¤ì¹­(^íŒ¨í„´$)ì„ ì„ í˜¸í•˜ì„¸ìš”**

**ì•ˆì „í•˜ê²Œ ì œê±° ê°€ëŠ¥í•œ ë…¸ì´ì¦ˆ:**
  * UI ìš”ì†Œ: "ì €ì¥ ì¸ì‡„ ë³´ê´€" (ì •í™•í•œ ë¬¸êµ¬ë§Œ)
  * ì‹œìŠ¤í…œ ë©”ë‰´: "PDFë¡œ ë³´ê¸°" (ì •í™•í•œ ë¬¸êµ¬ë§Œ)
  * í˜ì´ì§€ ë²ˆí˜¸: "í˜ì´ì§€ 123" (êµ¬ì²´ì  íŒ¨í„´ë§Œ)
  * êµ¬ë¶„ì„ : "-----" (ì •í™•í•œ íŒ¨í„´ë§Œ)
  * ì†Œì†¡ë¹„ìš©: "ì†Œì†¡ë¹„ìš©ì€...ë¶€ë‹´í•œë‹¤." (êµ¬ì²´ì  ë¬¸ì¥ë§Œ)
  * ì„¹ì…˜ ì œëª©: "ã€ì£¼ ë¬¸ã€‘" (ì œëª©ë§Œ, ë‚´ìš©ì€ ë³´ì¡´)

**ë³´ì¡´í•´ì•¼ í•  ì‚¬ì‹¤ê´€ê³„:**
  * ë‹¹ì‚¬ì ì •ë³´ (ëˆ„ê°€)
  * ì‚¬ê±´ ë°œìƒ ê²½ìœ„ (ì–¸ì œ, ì–´ë””ì„œ, ë¬´ì—‡ì„)
  * êµ¬ì²´ì  í–‰ìœ„ë‚˜ ì‚¬ê±´ (ì–´ë–»ê²Œ)
  * ê°ê´€ì  ì‚¬ì‹¤ì´ë‚˜ ì¦ê±°

**í‰ê°€ ê¸°ì¤€:**
1. NRR (Noise Reduction Rate): ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ ì œê±°ìœ¨ (0-1)
2. ICR (Important Content Retention): ì¤‘ìš”í•œ ì‚¬ì‹¤ ë³´ì¡´ìœ¨ (0-1)
3. SS (Semantic Similarity): ì˜ë¯¸ ìœ ì‚¬ì„± ìœ ì§€ ì •ë„ (0-1)
4. í† í° ì ˆê°ë¥ : ì „ì²˜ë¦¬ë¡œ ì¸í•œ í† í° ìˆ˜ ê°ì†Œ ë¹„ìœ¨ (%)
5. parsing_errors: íŒŒì‹± ê³¼ì •ì—ì„œ ë°œìƒí•œ ì˜¤ë¥˜ ê°œìˆ˜

**ì¤‘ìš”: suggestions ë°°ì—´ì—ëŠ” ë°œê²¬í•œ ëª¨ë“  ê°œì„  ì œì•ˆì„ í¬í•¨í•˜ì„¸ìš”. ë¹ˆ ë°°ì—´ë¡œ ë‘ì§€ ë§ˆì„¸ìš”.**

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:

{{
    "metrics": {{
        "nrr": 0.85,
        "icr": 0.92,
        "ss": 0.88,
        "token_reduction": 22.3,
        "parsing_errors": 0
    }},
    "errors": [
        "ì œê±°ë˜ì§€ ì•Šì€ í˜ì´ì§€ ë²ˆí˜¸ íŒ¨í„´ ë°œê²¬",
        "ì¤‘ìš”í•œ ë‚ ì§œ ì •ë³´ê°€ ê³¼ë„í•˜ê²Œ ì¶•ì•½ë¨"
    ],
    "suggestions": [
        {{
            "description": "í˜ì´ì§€ ë²ˆí˜¸ ì œê±°",
            "confidence_score": 0.95,
            "rule_type": "noise_removal",
            "estimated_improvement": "í˜ì´ì§€ ë²ˆí˜¸ ì œê±°ë¡œ 3-5% ê°„ì†Œí™”",
            "applicable_cases": ["ëª¨ë“  ë¬¸ì„œ"],
            "pattern_before": "^í˜ì´ì§€ \\d+$",
            "pattern_after": ""
        }},
        {{
            "description": "ì†Œì†¡ë¹„ìš© ë¶€ë‹´ ë¬¸êµ¬ ì œê±°",
            "confidence_score": 0.90,
            "rule_type": "noise_removal",
            "estimated_improvement": "ë¹„ìš© ë¶€ë‹´ ë¬¸êµ¬ ì œê±°",
            "applicable_cases": ["ëª¨ë“  ë¬¸ì„œ"],
            "pattern_before": "ì†Œì†¡ë¹„ìš©ì€.*?ë¶€ë‹´í•œë‹¤\\.",
            "pattern_after": ""
        }},
        {{
            "description": "UI ìš”ì†Œ ì œê±°",
            "confidence_score": 0.88,
            "rule_type": "noise_removal",
            "estimated_improvement": "UI ë…¸ì´ì¦ˆ ì œê±°ë¡œ 3-5% ê°„ì†Œí™”",
            "applicable_cases": ["ëª¨ë“  ë¬¸ì„œ"],
            "pattern_before": "ì €ì¥ ì¸ì‡„ ë³´ê´€ ì „ìíŒ©ìŠ¤ ê³µìœ  í™”ë©´ë‚´ ê²€ìƒ‰ ì¡°íšŒ ë‹«ê¸°",
            "pattern_after": ""
        }}
    ]
}}
"""
    
    def _create_improvement_prompt(self, pattern: Dict[str, Any]) -> str:
        """ê°œì„  ì œì•ˆ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        return f"""
ë‹¤ìŒ ì‹¤íŒ¨ íŒ¨í„´ì„ ë¶„ì„í•˜ê³  ê°œì„  ë°©ì•ˆì„ ì œì•ˆí•´ì£¼ì„¸ìš”.

**ì‹¤íŒ¨ íŒ¨í„´:**
- ì˜¤ë¥˜ ë©”ì‹œì§€: {pattern['_id']}
- ë°œìƒ íšŸìˆ˜: {pattern['count']}
- ìƒ˜í”Œ ì¼€ì´ìŠ¤: {pattern.get('sample_cases', [])[:3]}

**ìš”êµ¬ì‚¬í•­:**
1. ì˜¤ë¥˜ì˜ ê·¼ë³¸ ì›ì¸ ë¶„ì„
2. ì •ê·œì‹ íŒ¨í„´ ê°œì„  ì œì•ˆ
3. ì ìš© ìš°ì„ ìˆœìœ„ ë° ì‹ ë¢°ë„

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "rule_type": "page_number_removal",
    "description": "ê°œì„  ì„¤ëª…",
    "pattern": "ì •ê·œì‹ íŒ¨í„´",
    "replacement": "ëŒ€ì²´ ë¬¸ìì—´",
    "confidence_score": 0.85,
    "priority": 100
}}
"""
    
    def _parse_evaluation_result(
        self, 
        result_text: str, 
        before_content: str, 
        after_content: str
    ) -> Tuple[QualityMetrics, List[str], str]:
        """í‰ê°€ ê²°ê³¼ íŒŒì‹±"""
        
        try:
            print(f"ğŸ” DEBUG: Attempting to parse JSON: {result_text}")
            
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (```jsonê³¼ ``` ì‚¬ì´ì˜ ë‚´ìš©)
            json_text = result_text
            if "```json" in result_text:
                start = result_text.find("```json") + 7
                end = result_text.find("```", start)
                if end != -1:
                    json_text = result_text[start:end].strip()
                    print(f"ğŸ” DEBUG: Extracted JSON: {json_text}")
            elif "{" in result_text and "}" in result_text:
                # JSON ë§ˆì»¤ê°€ ì—†ëŠ” ê²½ìš°, ì²« ë²ˆì§¸ { ë¶€í„° ë§ˆì§€ë§‰ } ê¹Œì§€ ì¶”ì¶œ
                start = result_text.find("{")
                end = result_text.rfind("}") + 1
                json_text = result_text[start:end].strip()
                print(f"ğŸ” DEBUG: Extracted JSON (fallback): {json_text}")
            
            # ì •ê·œì‹ íŒ¨í„´ì˜ ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì ì²˜ë¦¬
            try:
                result_data = json.loads(json_text)
            except json.JSONDecodeError as json_error:
                print(f"ğŸ” DEBUG: JSON íŒŒì‹± ì˜¤ë¥˜, ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì ì²˜ë¦¬ ì‹œë„: {json_error}")
                # ì •ê·œì‹ íŒ¨í„´ì—ì„œ ë°±ìŠ¬ë˜ì‹œë¥¼ ì´ì¤‘ ë°±ìŠ¬ë˜ì‹œë¡œ ë³€í™˜
                fixed_json_text = json_text
                # pattern_beforeì™€ pattern_after í•„ë“œì—ì„œ ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì ìˆ˜ì •
                import re as regex_module
                pattern_fields = regex_module.findall(r'"pattern_before":\s*"([^"]*)"', fixed_json_text)
                for pattern in pattern_fields:
                    if '\\' in pattern and not '\\\\' in pattern:
                        # ë‹¨ì¼ ë°±ìŠ¬ë˜ì‹œë¥¼ ì´ì¤‘ ë°±ìŠ¬ë˜ì‹œë¡œ ë³€ê²½
                        fixed_pattern = pattern.replace('\\', '\\\\')
                        fixed_json_text = fixed_json_text.replace(f'"pattern_before": "{pattern}"', f'"pattern_before": "{fixed_pattern}"')
                
                pattern_after_fields = regex_module.findall(r'"pattern_after":\s*"([^"]*)"', fixed_json_text)
                for pattern in pattern_after_fields:
                    if '\\' in pattern and not '\\\\' in pattern:
                        # ë‹¨ì¼ ë°±ìŠ¬ë˜ì‹œë¥¼ ì´ì¤‘ ë°±ìŠ¬ë˜ì‹œë¡œ ë³€ê²½
                        fixed_pattern = pattern.replace('\\', '\\\\')
                        fixed_json_text = fixed_json_text.replace(f'"pattern_after": "{pattern}"', f'"pattern_after": "{fixed_pattern}"')
                
                print(f"ğŸ” DEBUG: ìˆ˜ì •ëœ JSON: {fixed_json_text[:500]}...")
                result_data = json.loads(fixed_json_text)
            
            metrics = QualityMetrics(
                nrr=result_data["metrics"]["nrr"],
                fpr=result_data["metrics"]["icr"],  # ICRì„ fpr í•„ë“œì— ì €ì¥ (ê¸°ì¡´ í˜¸í™˜ì„±)
                ss=result_data["metrics"]["ss"],
                token_reduction=result_data["metrics"]["token_reduction"],
                parsing_errors=result_data["metrics"].get("parsing_errors", 0)
            )
            
            errors = result_data.get("errors", [])
            suggestions = result_data.get("suggestions", [])
            
            print(f"ğŸ” DEBUG: AI ì‘ë‹µ ìƒì„¸ ë¶„ì„:")
            print(f"  - ë©”íŠ¸ë¦­ìŠ¤: NRR={metrics.nrr}, ICR/FPR={metrics.fpr}, SS={metrics.ss}, Token reduction={metrics.token_reduction}%")
            print(f"  - ì˜¤ë¥˜ ê°œìˆ˜: {len(errors)}")
            print(f"  - ì œì•ˆ ê°œìˆ˜: {len(suggestions)}")
            if suggestions:
                print(f"  - ì œì•ˆ ë‚´ìš©: {suggestions}")
            else:
                print(f"  - âš ï¸ AIê°€ ì œì•ˆì„ ìƒì„±í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
                print(f"  - ì›ë³¸ suggestions ë°ì´í„°: {result_data.get('suggestions', 'KEY_NOT_FOUND')}")
            
            return metrics, errors, suggestions
            
        except Exception as e:
            logger.error(f"Failed to parse evaluation result: {e}")
            logger.error(f"Raw result text: {result_text}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            metrics = QualityMetrics(nrr=0.0, fpr=0.0, ss=0.0, token_reduction=0.0)
            return metrics, [f"íŒŒì‹± ì˜¤ë¥˜: {str(e)}"], []
    
    def _parse_improvement_suggestion(
        self, 
        suggestion_text: str, 
        pattern: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """ê°œì„  ì œì•ˆ íŒŒì‹±"""
        
        try:
            suggestion_data = json.loads(suggestion_text)
            
            return {
                "patch_id": f"ai_suggestion_{pattern['_id'][:10]}",
                "rule_type": suggestion_data.get("rule_type"),
                "description": suggestion_data.get("description"),
                "pattern": suggestion_data.get("pattern"),
                "replacement": suggestion_data.get("replacement", ""),
                "confidence_score": suggestion_data.get("confidence_score", 0.5),
                "priority": suggestion_data.get("priority", 50),
                "applicable_cases": pattern.get("sample_cases", [])[:5]
            }
            
        except Exception as e:
            logger.warning(f"Failed to parse improvement suggestion: {e}")
            return None
    
    async def _create_batch_file(self, requests: List[Dict[str, Any]]) -> Any:
        """ë°°ì¹˜ íŒŒì¼ ìƒì„±"""
        import io
        
        print(f"ğŸ” DEBUG: ë°°ì¹˜ íŒŒì¼ ìƒì„± ì‹œì‘ - {len(requests)}ê°œ ìš”ì²­")
        
        # JSONL í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        jsonl_content = "\n".join(json.dumps(req) for req in requests)
        
        print(f"ğŸ” DEBUG: JSONL ì½˜í…ì¸  ìƒì„± ì™„ë£Œ - {len(jsonl_content)} ë¬¸ì")
        
        # BytesIO ê°ì²´ë¡œ íŒŒì¼ ìƒì„±
        file_obj = io.BytesIO(jsonl_content.encode('utf-8'))
        file_obj.name = 'batch_requests.jsonl'  # íŒŒì¼ëª… ì„¤ì •
        
        print(f"ğŸ” DEBUG: íŒŒì¼ ê°ì²´ ìƒì„± ì™„ë£Œ - {file_obj.name}")
        
        # íŒŒì¼ ì—…ë¡œë“œ
        try:
            file_response = await self.client.files.create(
                file=file_obj,
                purpose="batch"
            )
            
            print(f"âœ… DEBUG: ë°°ì¹˜ íŒŒì¼ ì—…ë¡œë“œ ì„±ê³µ - ID: {file_response.id}")
            return file_response
            
        except Exception as e:
            print(f"âŒ DEBUG: ë°°ì¹˜ íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    async def _wait_for_batch_completion(self, batch_id: str, max_wait_time: int = 3600) -> Any:
        """ë°°ì¹˜ ì™„ë£Œ ëŒ€ê¸°"""
        
        wait_time = 0
        while wait_time < max_wait_time:
            batch = await self.client.batches.retrieve(batch_id)
            
            if batch.status == "completed":
                return batch
            elif batch.status == "failed":
                raise Exception(f"Batch job failed: {batch.errors}")
            
            await asyncio.sleep(30)  # 30ì´ˆ ëŒ€ê¸°
            wait_time += 30
        
        raise Exception("Batch job timeout")
    
    async def _parse_batch_results(
        self, 
        batch_result: Any, 
        original_cases: List[Dict[str, Any]]
    ) -> List[Tuple[str, QualityMetrics, List[str], str]]:
        """ë°°ì¹˜ ê²°ê³¼ íŒŒì‹±"""
        
        results = []
        
        # ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        if batch_result.output_file_id:
            output_file = await self.client.files.content(batch_result.output_file_id)
            output_content = output_file.read().decode()
            
            for line in output_content.strip().split('\n'):
                try:
                    result_data = json.loads(line)
                    custom_id = result_data["custom_id"]
                    
                    # case_id ì¶”ì¶œ
                    case_id = custom_id.split('_')[1]
                    
                    # ì›ë³¸ ì¼€ì´ìŠ¤ ì°¾ê¸°
                    original_case = next(
                        (case for case in original_cases if case["case_id"] == case_id), 
                        None
                    )
                    
                    if original_case:
                        response_content = result_data["response"]["body"]["choices"][0]["message"]["content"]
                        
                        metrics, errors, suggestions = self._parse_evaluation_result(
                            response_content,
                            original_case["before_content"],
                            original_case["after_content"]
                        )
                        
                        results.append((case_id, metrics, errors, suggestions))
                        
                except Exception as e:
                    logger.warning(f"Failed to parse batch result line: {e}")
                    continue
        
        return results
    
    def calculate_token_count(self, text: str) -> int:
        """í† í° ìˆ˜ ê³„ì‚° (ê·¼ì‚¬ì¹˜)"""
        # ê°„ë‹¨í•œ í† í° ìˆ˜ ì¶”ì • (ì‹¤ì œë¡œëŠ” tiktoken ë“±ì„ ì‚¬ìš©)
        return len(text.split()) * 1.3  # í•œêµ­ì–´ íŠ¹ì„± ê³ ë ¤
    
    async def estimate_batch_cost(self, cases: List[Dict[str, Any]]) -> float:
        """ë°°ì¹˜ ë¹„ìš© ì¶”ì •"""
        
        total_tokens = 0
        
        for case in cases:
            prompt = self._create_evaluation_prompt(
                case["before_content"], 
                case["after_content"], 
                case.get("metadata", {})
            )
            total_tokens += self.calculate_token_count(prompt)
        
        # GPT-4 Turbo ê°€ê²© ê¸°ì¤€ (input: $0.01/1K tokens, output: $0.03/1K tokens)
        input_cost = (total_tokens / 1000) * 0.01
        output_cost = (len(cases) * 500 / 1000) * 0.03  # í‰ê·  500 í† í° ì¶œë ¥ ê°€ì •
        
        return input_cost + output_cost
