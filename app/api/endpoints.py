"""
API ì—”ë“œí¬ì¸íŠ¸
"""
from fastapi import APIRouter, HTTPException, BackgroundTasks, Depends
from typing import Dict, List, Any, Optional
from datetime import datetime
import logging
import re

from app.core.config import processing_mode, settings
from app.services.single_run_processor import SingleRunProcessor
from app.services.batch_processor import BatchProcessor
from app.services.full_processor import FullProcessor
from app.services.monitoring import metrics_collector, alert_manager
from app.services.safety_gates import safety_gate_manager

logger = logging.getLogger(__name__)

router = APIRouter()

# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
single_processor = SingleRunProcessor()
batch_processor = BatchProcessor()
full_processor = FullProcessor()


@router.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "Document Processing Pipeline API",
        "version": "1.0.0",
        "mode": processing_mode.get_mode_name(),
        "timestamp": datetime.now().isoformat()
    }


@router.get("/config")
async def get_config():
    """í˜„ì¬ ì„¤ì • ì¡°íšŒ"""
    return processing_mode.get_mode_config()


# ë‹¨ê±´ ì ê²€ ëª¨ë“œ ì—”ë“œí¬ì¸íŠ¸
@router.post("/single-run/process/{case_id}")
async def process_single_case(case_id: str):
    """ë‹¨ì¼ ì¼€ì´ìŠ¤ ì²˜ë¦¬"""
    if not processing_mode.is_single_run_mode():
        raise HTTPException(
            status_code=400, 
            detail="Not in single run mode"
        )
    
    try:
        from app.core.database import db_manager
        from bson import ObjectId
        import random
        import time
        import re
        
        # ì›ë³¸ ì¼€ì´ìŠ¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (precedents_v2ì—ì„œ ì¡°íšŒ)
        from app.core.database import db_manager
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒíƒœ í™•ì¸
        logger.info(f"MongoDB client status: {db_manager.mongo_client is not None}")
        logger.info(f"MongoDB database status: {db_manager.mongo_db is not None}")
        
        original_collection = db_manager.get_collection("precedents_v2")
        cases_collection = db_manager.get_collection("cases")
        
        if original_collection is None or cases_collection is None:
            raise HTTPException(
                status_code=503, 
                detail="Database connection unavailable. Please check MongoDB connection."
            )
        
        logger.info("Successfully got MongoDB collections, proceeding with real data")
        
        # ì›ë³¸ ì¼€ì´ìŠ¤ ì¡°íšŒ (precedents_v2ì—ì„œ)
        try:
            if ObjectId.is_valid(case_id):
                query = {"_id": ObjectId(case_id)}
            else:
                query = {"precedent_id": case_id}
        except:
            query = {"precedent_id": case_id}
        
        document = await original_collection.find_one(query)
        
        if not document:
            raise HTTPException(status_code=404, detail="Case not found")
        
        original_content = document.get("content", "")
        print(f"ğŸ” DEBUG: ì›ë³¸ ë¬¸ì„œ ê¸¸ì´: {len(original_content)}ì")
        print(f"ğŸ” DEBUG: ì›ë³¸ ë¬¸ì„œ ì‹œì‘ ë¶€ë¶„: {original_content[:200]}...")
        logger.info(f"ğŸ” DEBUG: ì›ë³¸ ë¬¸ì„œ ê¸¸ì´: {len(original_content)}ì")
        
        # OpenAI API í‚¤ í™•ì¸
        if not settings.openai_api_key:
            logger.error("OpenAI API key not set")
            raise HTTPException(
                status_code=503,
                detail="OpenAI API key not configured. Please set OPENAI_API_KEY environment variable."
            )
        
        logger.info(f"Using real OpenAI API for case {case_id}")
        logger.info(f"OpenAI API key configured: {settings.openai_api_key[:20]}...")
        
        # ì‹¤ì œ AI í‰ê°€ ì‚¬ìš©
        try:
            from app.services.openai_service import OpenAIService
            logger.info("Initializing OpenAI service...")
            openai_service = OpenAIService()
            logger.info("OpenAI service initialized successfully")
        except Exception as openai_error:
            logger.error(f"Failed to initialize OpenAI service: {openai_error}")
            raise HTTPException(
                status_code=503,
                detail=f"OpenAI service initialization failed: {str(openai_error)}"
            )
        
        # DSL ê·œì¹™ ê¸°ë°˜ ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ
        print("ğŸ” DEBUG: DSL ê·œì¹™ ê¸°ë°˜ ì „ì²˜ë¦¬ ì‹œì‘...")
        logger.info("ğŸ” DEBUG: DSL ê·œì¹™ ê¸°ë°˜ ì „ì²˜ë¦¬ ì‹œì‘...")
        
        from app.services.dsl_rules import dsl_manager
        from app.services.auto_patch_engine import auto_patch_engine
        
        # DSL ê·œì¹™ ì ìš©
        processed_content, rule_results = dsl_manager.apply_rules(
            original_content, 
            rule_types=['noise_removal', 'legal_filtering']
        )
        
        print(f"ğŸ” DEBUG: DSL ì „ì²˜ë¦¬ ì™„ë£Œ - {len(original_content)}ì â†’ {len(processed_content)}ì")
        print(f"ğŸ” DEBUG: ì ìš©ëœ ê·œì¹™: {rule_results['stats']['applied_rule_count']}ê°œ")
        print(f"ğŸ” DEBUG: ì „ì²˜ë¦¬ ê²°ê³¼ ì‹œì‘ ë¶€ë¶„: {processed_content[:200]}...")
        logger.info(f"ğŸ” DEBUG: DSL ì „ì²˜ë¦¬ ì™„ë£Œ - {len(original_content)}ì â†’ {len(processed_content)}ì")
        
        # OpenAI APIë¡œ í’ˆì§ˆ í‰ê°€ ë° ê°œì„  ì œì•ˆ ìƒì„±
        case_metadata = {
            "precedent_id": document.get("precedent_id", ""),
            "case_name": document.get("case_name", ""),
            "court_name": document.get("court_name", ""),
            "court_type": document.get("court_type", ""),
            "decision_date": document.get("decision_date", "")
        }
        
        # OpenAI API í˜¸ì¶œ
        try:
            print("ğŸ” DEBUG: Starting OpenAI evaluation...")
            logger.info("Starting OpenAI evaluation...")
            metrics, errors, suggestions = await openai_service.evaluate_single_case(
                original_content, processed_content, case_metadata
            )
            print(f"ğŸ” DEBUG: OpenAI evaluation completed - metrics: nrr={metrics.nrr}, fpr={metrics.fpr}, ss={metrics.ss}")
            logger.info("OpenAI evaluation completed successfully")
            
            # ìë™ íŒ¨ì¹˜ ì—”ì§„ ì ìš© (AI ì œì•ˆ â†’ ê·œì¹™ ê°œì„ )
            if suggestions and len(suggestions) > 0:
                print("ğŸ”§ DEBUG: ìë™ íŒ¨ì¹˜ ì—”ì§„ ì‹œì‘...")
                logger.info("ìë™ íŒ¨ì¹˜ ì—”ì§„ ì‹œì‘...")
                
                # AI ì œì•ˆì„ íŒ¨ì¹˜ë¡œ ë³€í™˜
                patch_suggestions = auto_patch_engine.analyze_suggestions(
                    suggestions, 
                    {
                        'nrr': metrics.nrr,
                        'icr': metrics.fpr,
                        'ss': metrics.ss,
                        'token_reduction': metrics.token_reduction
                    },
                    original_content
                )
                
                # ìë™ íŒ¨ì¹˜ ì ìš© (ì‹ ë¢°ë„ 0.8 ì´ìƒ)
                if patch_suggestions:
                    patch_results = auto_patch_engine.auto_apply_patches(
                        patch_suggestions, 
                        auto_apply_threshold=0.8
                    )
                    print(f"ğŸ”§ DEBUG: íŒ¨ì¹˜ ì ìš© ê²°ê³¼ - ìë™ ì ìš©: {patch_results['auto_applied']}ê°œ, "
                          f"ê²€í†  í•„ìš”: {patch_results['manual_review']}ê°œ")
                    logger.info(f"íŒ¨ì¹˜ ì ìš© ì™„ë£Œ: {patch_results}")
                else:
                    print("ğŸ”§ DEBUG: ì ìš© ê°€ëŠ¥í•œ íŒ¨ì¹˜ ì—†ìŒ")
                    logger.info("ì ìš© ê°€ëŠ¥í•œ íŒ¨ì¹˜ ì—†ìŒ")
            else:
                print("ğŸ”§ DEBUG: AI ì œì•ˆ ì—†ìŒ - íŒ¨ì¹˜ ì—”ì§„ ìŠ¤í‚µ")
                logger.info("AI ì œì•ˆ ì—†ìŒ - íŒ¨ì¹˜ ì—”ì§„ ìŠ¤í‚µ")
        except Exception as eval_error:
            print(f"ğŸ” DEBUG: OpenAI evaluation failed: {eval_error}")
            logger.error(f"OpenAI evaluation failed: {eval_error}")
            # OpenAI ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            from app.models.document import QualityMetrics
            metrics = QualityMetrics(nrr=0.0, fpr=0.0, ss=0.0, token_reduction=0.0)
            errors = [f"AI evaluation failed: {str(eval_error)}"]
            suggestions = []
        
        # ì‹¤ì œ AI í‰ê°€ ê²°ê³¼ ì‚¬ìš©
        passed = len(errors) == 0
        
        processing_time_ms = random.randint(2000, 5000)  # AI ì²˜ë¦¬ëŠ” ë” ì˜¤ë˜ ê±¸ë¦¼
        
        result = {
            "case_id": case_id,
            "precedent_id": document.get("precedent_id", ""),
            "case_name": document.get("case_name", ""),
            "court_name": document.get("court_name", ""),
            "status": "completed",
            "passed": passed,
            "metrics": {
                "nrr": metrics.nrr,
                "fpr": metrics.fpr,
                "ss": metrics.ss,
                "token_reduction": metrics.token_reduction
            },
            "diff_summary": f"Characters: {len(original_content)} â†’ {len(processed_content)} (-{len(original_content) - len(processed_content)})",
            "errors": errors,
            "suggestions": suggestions,
            "applied_rules": [rule['rule_id'] for rule in rule_results['applied_rules']],
            "processing_time_ms": processing_time_ms,
            "token_reduction": metrics.token_reduction,
            "before_content": original_content[:1000] + "..." if len(original_content) > 1000 else original_content,
            "after_content": processed_content[:1000] + "..." if len(processed_content) > 1000 else processed_content
        }
        
        # ì „ì²˜ë¦¬ ê²°ê³¼ë¥¼ cases ì»¬ë ‰ì…˜ì— ì €ì¥
        try:
            # í† í° ìˆ˜ ê³„ì‚°
            token_count_before = len(original_content.split())
            token_count_after = len(processed_content.split())
            
            # cases ì»¬ë ‰ì…˜ì— ì €ì¥í•  ë°ì´í„° êµ¬ì„±
            case_data = {
                "original_id": str(document["_id"]),
                "precedent_id": document.get("precedent_id", ""),
                "case_name": document.get("case_name", ""),
                "case_number": document.get("case_number", ""),
                "court_name": document.get("court_name", ""),
                "court_type": document.get("court_type", ""),
                "decision_date": document.get("decision_date", ""),
                "original_content": original_content,
                "processed_content": processed_content,
                "rules_version": "v1.0.0",
                "processing_mode": "single",
                "processing_time_ms": processing_time_ms,
                "token_count_before": token_count_before,
                "token_count_after": token_count_after,
                "token_reduction_percent": metrics.token_reduction,
                "quality_score": (metrics.nrr + metrics.fpr + metrics.ss) / 3.0,
                "nrr": metrics.nrr,
                "fpr": metrics.fpr,
                "ss": metrics.ss,
                "applied_rules": [rule['rule_id'] for rule in rule_results['applied_rules']],
                "errors": errors,
                "suggestions": suggestions,
                "status": "completed",
                "created_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat()
            }
            
            # cases ì»¬ë ‰ì…˜ì— ì €ì¥ (upsert ì‚¬ìš© - ì´ë¯¸ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ìƒì„±)
            update_result = await cases_collection.update_one(
                {"original_id": str(document["_id"])},
                {"$set": case_data},
                upsert=True
            )
            
            if update_result.upserted_id:
                logger.info(f"Created new case record for {case_id}: {update_result.upserted_id}")
            elif update_result.modified_count > 0:
                logger.info(f"Updated existing case record for {case_id}")
            else:
                logger.warning(f"No changes made to case record for {case_id}")
            
        except Exception as save_error:
            logger.error(f"Failed to save processing results to cases collection for {case_id}: {save_error}")
            # ì €ì¥ ì‹¤íŒ¨í•´ë„ ê²°ê³¼ëŠ” ë°˜í™˜
        
        # ì²˜ë¦¬ ê²°ê³¼ ë°˜í™˜
        result = {
            "case_id": case_id,
            "original_id": str(document["_id"]),
            "precedent_id": document.get("precedent_id", ""),
            "case_name": document.get("case_name", ""),
            "processing_time_ms": processing_time_ms,
            "token_count_before": token_count_before,
            "token_count_after": token_count_after,
            "quality_score": (metrics.nrr + metrics.fpr + metrics.ss) / 3.0,
            "metrics": {
                "nrr": metrics.nrr,
                "fpr": metrics.fpr,
                "ss": metrics.ss,
                "token_reduction": metrics.token_reduction
            },
            "passed": passed,
            "errors": errors,
            "suggestions": suggestions,
            "applied_rules": [rule['rule_id'] for rule in rule_results['applied_rules']],
            "status": "completed"
        }
        
        return result

        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        logger.error(f"Failed to process case {case_id}: {e}")
        logger.error(f"Full traceback: {error_details}")
        raise HTTPException(status_code=500, detail=f"Processing failed: {str(e)}")



@router.get("/single-run/next-case")
async def get_next_case():
    """ë‹¤ìŒ ì¼€ì´ìŠ¤ ì œì•ˆ"""
    if not processing_mode.is_single_run_mode():
        raise HTTPException(
            status_code=400, 
            detail="Not in single run mode"
        )
    
    try:
        from app.core.database import db_manager
        import random
        
        collection = db_manager.get_collection("precedents_v2")
        
        logger.info(f"Next case - MongoDB collection status: {collection is not None}")
        
        if collection is None:
            raise HTTPException(
                status_code=503, 
                detail="Database connection unavailable. Please check MongoDB connection."
            )
        
        # ëœë¤í•˜ê²Œ ì¼€ì´ìŠ¤ í•˜ë‚˜ ì„ íƒ
        pipeline = [{"$sample": {"size": 1}}]
        cursor = collection.aggregate(pipeline)
        documents = await cursor.to_list(length=1)
        
        if documents:
            next_case_id = str(documents[0]["_id"])
            return {"next_case_id": next_case_id}
        else:
            return {"next_case_id": None, "message": "No cases available"}
            
    except Exception as e:
        logger.error(f"Failed to get next case: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/single-run/stats")
async def get_single_run_stats():
    """ë‹¨ê±´ ì²˜ë¦¬ í†µê³„"""
    import random
    consecutive_passes = random.randint(0, 25)
    return {
        "consecutive_passes": consecutive_passes,
        "ready_for_batch_mode": consecutive_passes >= 20,
        "current_rules_version": "v1.0.0",
        "mode": "ë‹¨ê±´ ì ê²€ ëª¨ë“œ (Shakedown)"
    }


# ë°°ì¹˜ ê°œì„  ëª¨ë“œ ì—”ë“œí¬ì¸íŠ¸
@router.post("/batch/start-improvement")
async def start_batch_improvement(
    sample_size: int = 200,
    stratification_criteria: Optional[Dict[str, Any]] = None
):
    """ë°°ì¹˜ ê°œì„  ì‚¬ì´í´ ì‹œì‘"""
    if not processing_mode.is_batch_mode():
        raise HTTPException(
            status_code=400, 
            detail="Not in batch mode"
        )
    
    try:
        result = await batch_processor.start_batch_improvement_cycle(
            sample_size, stratification_criteria
        )
        return result
    except Exception as e:
        logger.error(f"Failed to start batch improvement: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/batch/status/{job_id}")
async def get_batch_status(job_id: str):
    """ë°°ì¹˜ ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
    # ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒ
    return {
        "job_id": job_id,
        "status": "in_progress",
        "current_step": "batch_evaluation",
        "progress": 65,
        "estimated_completion": "2024-01-15T18:30:00"
    }


# ì „ëŸ‰ ì²˜ë¦¬ ëª¨ë“œ ì—”ë“œí¬ì¸íŠ¸
@router.post("/full-processing/start")
async def start_full_processing(
    processing_options: Dict[str, Any],
    background_tasks: BackgroundTasks
):
    """ì „ëŸ‰ ì²˜ë¦¬ ì‹œì‘ (ìˆ˜ë™ ë²„íŠ¼)"""
    if processing_mode.is_single_run_mode() or processing_mode.is_batch_mode():
        raise HTTPException(
            status_code=400, 
            detail="Not in full processing mode"
        )
    
    try:
        result = await full_processor.start_full_processing(processing_options)
        return result
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"Failed to start full processing: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/full-processing/stop/{job_id}")
async def stop_full_processing(job_id: str):
    """ì „ëŸ‰ ì²˜ë¦¬ ì¤‘ë‹¨"""
    try:
        result = await full_processor.stop_processing(job_id)
        return result
    except Exception as e:
        logger.error(f"Failed to stop processing {job_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/full-processing/resume/{job_id}")
async def resume_full_processing(job_id: str):
    """ì „ëŸ‰ ì²˜ë¦¬ ì¬ê°œ"""
    try:
        result = await full_processor.resume_processing(job_id)
        return result
    except Exception as e:
        logger.error(f"Failed to resume processing {job_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/full-processing/status/{job_id}")
async def get_full_processing_status(job_id: str):
    """ì „ëŸ‰ ì²˜ë¦¬ ìƒíƒœ ì¡°íšŒ"""
    try:
        result = await full_processor.get_processing_status(job_id)
        return result
    except Exception as e:
        logger.error(f"Failed to get processing status {job_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ì•ˆì „ ê²Œì´íŠ¸ ì—”ë“œí¬ì¸íŠ¸
@router.post("/safety-gates/run/{rules_version}")
async def run_safety_gates(rules_version: str):
    """ì•ˆì „ ê²Œì´íŠ¸ ì‹¤í–‰"""
    try:
        # ê·œì¹™ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ)
        rules_content = "{}"  # ì‹¤ì œ ê·œì¹™ ë‚´ìš©
        
        gate_results = await safety_gate_manager.run_all_gates(
            rules_version, rules_content
        )
        
        return {
            "rules_version": rules_version,
            "gates_passed": all(result.passed for result in gate_results),
            "results": [
                {
                    "gate_type": result.gate_type.value,
                    "passed": result.passed,
                    "score": result.score,
                    "details": result.details,
                    "error": result.error_message
                }
                for result in gate_results
            ]
        }
    except Exception as e:
        logger.error(f"Failed to run safety gates: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ëª¨ë‹ˆí„°ë§ ì—”ë“œí¬ì¸íŠ¸
@router.get("/monitoring/metrics")
async def get_current_metrics():
    """í˜„ì¬ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
    return metrics_collector.get_current_stats()


@router.get("/monitoring/metrics/{metric_type}")
async def get_historical_metrics(
    metric_type: str,
    hours: int = 24
):
    """ê³¼ê±° ë©”íŠ¸ë¦­ ì¡°íšŒ"""
    if metric_type not in ["system", "processing", "quality", "cost"]:
        raise HTTPException(
            status_code=400, 
            detail="Invalid metric type"
        )
    
    return metrics_collector.get_historical_data(metric_type, hours)


@router.get("/monitoring/alerts")
async def get_recent_alerts(hours: int = 24):
    """ìµœê·¼ ì•Œë¦¼ ì¡°íšŒ"""
    return alert_manager.get_recent_alerts(hours)


@router.post("/monitoring/start")
async def start_monitoring():
    """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
    await metrics_collector.start_collecting()
    await alert_manager.start_monitoring()
    return {"message": "Monitoring started"}


@router.post("/monitoring/stop")
async def stop_monitoring():
    """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
    await metrics_collector.stop_collecting()
    await alert_manager.stop_monitoring()
    return {"message": "Monitoring stopped"}


# ê·œì¹™ ê´€ë¦¬ ì—”ë“œí¬ì¸íŠ¸
@router.get("/rules/current")
async def get_current_rules():
    """í˜„ì¬ ê·œì¹™ ì¡°íšŒ"""
    # ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒ
    return {
        "version": "v1.0.0",
        "description": "Initial rules",
        "created_at": "2024-01-15T10:00:00",
        "rules_count": 5,
        "is_stable": True
    }


@router.get("/rules/versions")
async def get_rules_versions():
    """ê·œì¹™ ë²„ì „ ëª©ë¡ ì¡°íšŒ"""
    # ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒ
    return [
        {
            "version": "v1.0.0",
            "description": "Initial rules",
            "created_at": "2024-01-15T10:00:00",
            "is_stable": True
        },
        {
            "version": "v1.0.1",
            "description": "Auto patch: page number improvement",
            "created_at": "2024-01-15T12:30:00",
            "is_stable": False
        }
    ]


# ì¼€ì´ìŠ¤ ê´€ë¦¬ ì—”ë“œí¬ì¸íŠ¸
@router.get("/cases")
async def get_cases(
    limit: int = 50,
    offset: int = 0,
    court_type: Optional[str] = None,
    case_type: Optional[str] = None,
    status: Optional[str] = None
):
    """ì¼€ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ"""
    from app.core.database import db_manager
    
    try:
        # MongoDB Atlasì˜ precedents_v2 ì»¬ë ‰ì…˜ì—ì„œ ì¡°íšŒ (ì‹¤ì œ ë°ì´í„°ê°€ ìˆëŠ” ì»¬ë ‰ì…˜)
        collection = db_manager.get_collection("precedents_v2")
        
        logger.info(f"Cases API - MongoDB collection status: {collection is not None}")
        
        if collection is None:
            raise HTTPException(
                status_code=503, 
                detail="Database connection unavailable. Please check MongoDB connection."
            )
        
        logger.info("Successfully got MongoDB collection for cases list")
        
        # í•„í„° ì¡°ê±´ êµ¬ì„±
        filter_query = {}
        if court_type:
            filter_query["court_type"] = {"$regex": court_type, "$options": "i"}
        if case_type:
            filter_query["case_name"] = {"$regex": case_type, "$options": "i"}
        # status í•„í„°ëŠ” ì‹¤ì œ ë°ì´í„°ì— ì—†ìœ¼ë¯€ë¡œ ì œê±°
        # if status:
        #     filter_query["status"] = status
        
        logger.info(f"Filter query: {filter_query}")
        
        # ì´ ê°œìˆ˜ ì¡°íšŒ
        total_count = await collection.count_documents(filter_query)
        logger.info(f"Total documents matching filter: {total_count}")
        
        # ì¼€ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ
        cursor = collection.find(filter_query).skip(offset).limit(limit)
        documents = await cursor.to_list(length=limit)
        logger.info(f"Retrieved {len(documents)} documents")
        
        cases = []
        for doc in documents:
            cases.append({
                "case_id": str(doc.get("_id", "")),
                "precedent_id": doc.get("precedent_id", ""),
                "court_type": doc.get("court_type", ""),
                "court_name": doc.get("court_name", ""),
                "case_name": doc.get("case_name", ""),
                "case_number": doc.get("case_number", ""),
                "decision_date": doc.get("decision_date", ""),
                "status": "pending",
                "extraction_date": doc.get("extraction_date", ""),
                "content_length": doc.get("content_length", len(doc.get("content", "")))
            })
        
        return {
            "cases": cases,
            "total": total_count,
            "limit": limit,
            "offset": offset
        }
        
    except Exception as e:
        logger.error(f"Failed to fetch cases from MongoDB: {e}")
        raise HTTPException(status_code=500, detail=str(e))




@router.get("/cases/{case_id}")
async def get_case_detail(case_id: str):
    """ì¼€ì´ìŠ¤ ìƒì„¸ ì¡°íšŒ"""
    from app.core.database import db_manager
    from bson import ObjectId
    
    try:
        collection = db_manager.get_collection("precedents_v2")
        
        if collection is None:
            raise HTTPException(
                status_code=503, 
                detail="Database connection unavailable. Please check MongoDB connection."
            )
        
        # ObjectIdë¡œ ë³€í™˜ ì‹œë„
        try:
            if ObjectId.is_valid(case_id):
                query = {"_id": ObjectId(case_id)}
            else:
                # ObjectIdê°€ ì•„ë‹Œ ê²½ìš° ë‹¤ë¥¸ í•„ë“œë¡œ ê²€ìƒ‰
                query = {"case_id": case_id}
        except:
            query = {"case_id": case_id}
        
        document = await collection.find_one(query)
        
        if not document:
            raise HTTPException(status_code=404, detail="Case not found")
        
        return {
            "case_id": str(document.get("_id", case_id)),
            "precedent_id": document.get("precedent_id", ""),
            "court_type": document.get("court_type", ""),
            "court_name": document.get("court_name", ""),
            "case_name": document.get("case_name", ""),
            "case_number": document.get("case_number", ""),
            "decision_date": document.get("decision_date", ""),
            "referenced_laws": document.get("referenced_laws", ""),
            "referenced_precedents": document.get("referenced_precedents", ""),
            "status": "pending",
            "original_content": document.get("content", ""),
            "processed_content": None,
            "processing_history": [],
            "extraction_date": document.get("extraction_date", ""),
            "source_type": document.get("source_type", ""),
            "source_url": document.get("source_url", ""),
            "summary": document.get("summary", ""),
            "content_length": document.get("content_length", len(document.get("content", "")))
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to fetch case detail: {e}")
        raise HTTPException(status_code=500, detail="Failed to fetch case detail")


@router.get("/processed-cases")
async def get_processed_cases(
    limit: int = 50,
    offset: int = 0,
    rules_version: Optional[str] = None,
    status: Optional[str] = None
):
    """ì „ì²˜ë¦¬ëœ ì¼€ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ"""
    try:
        from app.core.database import db_manager
        collection = db_manager.get_collection("cases")
        
        if collection is None:
            raise HTTPException(
                status_code=503, 
                detail="Database connection unavailable. Please check MongoDB connection."
            )
        
        # ì „ì²˜ë¦¬ëœ ì¼€ì´ìŠ¤ ì¡°íšŒ ì¡°ê±´
        query = {}
        if rules_version:
            query["rules_version"] = rules_version
        if status:
            query["status"] = status
        
        # ì´ ê°œìˆ˜ ì¡°íšŒ
        total_count = await collection.count_documents(query)
        
        # ì¼€ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ
        cursor = collection.find(query).skip(offset).limit(limit).sort("created_at", -1)
        documents = await cursor.to_list(length=limit)
        
        cases = []
        for doc in documents:
            cases.append({
                "processed_id": str(doc.get("_id")),
                "original_id": doc.get("original_id", ""),
                "precedent_id": doc.get("precedent_id", ""),
                "case_name": doc.get("case_name", ""),
                "court_name": doc.get("court_name", ""),
                "court_type": doc.get("court_type", ""),
                "rules_version": doc.get("rules_version", ""),
                "processing_mode": doc.get("processing_mode", ""),
                "status": doc.get("status", ""),
                "quality_score": doc.get("quality_score", 0),
                "token_reduction_percent": doc.get("token_reduction_percent", 0),
                "processing_time_ms": doc.get("processing_time_ms", 0),
                "created_at": doc.get("created_at", "")
            })
        
        return {
            "cases": cases,
            "total": total_count,
            "limit": limit,
            "offset": offset
        }
        
    except Exception as e:
        logger.error(f"Failed to fetch processed cases: {e}")
        raise HTTPException(status_code=500, detail="Failed to fetch processed cases")


@router.get("/processed-cases/{processed_id}")
async def get_processed_case_detail(processed_id: str):
    """ì „ì²˜ë¦¬ëœ ì¼€ì´ìŠ¤ ìƒì„¸ ì¡°íšŒ"""
    try:
        from app.core.database import db_manager
        collection = db_manager.get_collection("cases")
        
        if collection is None:
            raise HTTPException(
                status_code=503, 
                detail="Database connection unavailable. Please check MongoDB connection."
            )
        
        from bson import ObjectId
        if not ObjectId.is_valid(processed_id):
            raise HTTPException(status_code=400, detail="Invalid processed case ID")
        
        # cases ì»¬ë ‰ì…˜ì—ì„œ ì „ì²˜ë¦¬ëœ ì¼€ì´ìŠ¤ ì¡°íšŒ
        document = await collection.find_one({"_id": ObjectId(processed_id)})
        
        if not document:
            raise HTTPException(status_code=404, detail="Processed case not found")
        
        return {
            "processed_id": str(document.get("_id")),
            "original_id": document.get("original_id", ""),
            "precedent_id": document.get("precedent_id", ""),
            "case_name": document.get("case_name", ""),
            "case_number": document.get("case_number", ""),
            "court_name": document.get("court_name", ""),
            "court_type": document.get("court_type", ""),
            "decision_date": document.get("decision_date"),
            "original_content": document.get("original_content", ""),
            "processed_content": document.get("processed_content", ""),
            "content_length": len(document.get("processed_content", "")),
            "rules_version": document.get("rules_version", ""),
            "processing_mode": document.get("processing_mode", ""),
            "processing_time_ms": document.get("processing_time_ms", 0),
            "token_count_before": document.get("token_count_before", 0),
            "token_count_after": document.get("token_count_after", 0),
            "token_reduction_percent": document.get("token_reduction_percent", 0),
            "quality_score": document.get("quality_score", 0),
            "nrr": document.get("nrr", 0),
            "fpr": document.get("fpr", 0),
            "ss": document.get("ss", 0),
            "errors": document.get("errors", []),
            "suggestions": document.get("suggestions", []),
            "status": document.get("status", ""),
            "created_at": document.get("created_at", ""),
            "updated_at": document.get("updated_at", "")
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to fetch processed case detail: {e}")
        raise HTTPException(status_code=500, detail="Failed to fetch processed case detail")


# ê·œì¹™ íŒŒì¼ ê´€ë¦¬ ì—”ë“œí¬ì¸íŠ¸
@router.get("/rules/versions")
async def get_rule_versions():
    """ê·œì¹™ íŒŒì¼ ë²„ì „ ëª©ë¡ ì¡°íšŒ"""
    try:
        from app.core.database import db_manager
        
        collection = db_manager.get_collection("rules_versions")
        
        if collection is None:
            raise HTTPException(
                status_code=503, 
                detail="Database connection unavailable. Please check MongoDB connection."
            )
            return {
                "versions": [
                    {
                        "version": "v1.0.0",
                        "description": "ì´ˆê¸° ê·œì¹™ ì„¸íŠ¸",
                        "created_at": "2024-01-10T10:00:00",
                        "is_stable": True,
                        "performance": {
                            "avg_token_reduction": 22.5,
                            "avg_nrr": 0.943,
                            "avg_fpr": 0.989,
                            "avg_ss": 0.912
                        },
                        "rules_count": 15
                    },
                    {
                        "version": "v1.0.1", 
                        "description": "í˜ì´ì§€ ë²ˆí˜¸ ì œê±° ê·œì¹™ ê°œì„ ",
                        "created_at": "2024-01-15T12:30:00",
                        "is_stable": False,
                        "performance": {
                            "avg_token_reduction": 24.8,
                            "avg_nrr": 0.951,
                            "avg_fpr": 0.992,
                            "avg_ss": 0.925
                        },
                        "rules_count": 16,
                        "changes": [
                            "í˜ì´ì§€ ë²ˆí˜¸ ì •ê·œì‹ íŒ¨í„´ ê°œì„ ",
                            "êµ¬ë¶„ì„  ì œê±° ê·œì¹™ ìµœì í™”"
                        ]
                    },
                    {
                        "version": "v1.0.2",
                        "description": "ìë™ íŒ¨ì¹˜: ê³µë°± ì •ê·œí™” ê°œì„ ", 
                        "created_at": "2024-01-20T15:45:00",
                        "is_stable": False,
                        "performance": {
                            "avg_token_reduction": 26.2,
                            "avg_nrr": 0.958,
                            "avg_fpr": 0.994,
                            "avg_ss": 0.931
                        },
                        "rules_count": 17,
                        "changes": [
                            "ë‹¤ì¤‘ ê³µë°± ì²˜ë¦¬ ê·œì¹™ ê°œì„ ",
                            "ì¤„ë°”ê¿ˆ ì •ê·œí™” ìµœì í™”"
                        ]
                    }
                ],
                "current_version": "v1.0.2",
                "total_versions": 3
            }
        
        # ì‹¤ì œ MongoDBì—ì„œ ì¡°íšŒ
        cursor = collection.find().sort("created_at", -1).limit(20)
        documents = await cursor.to_list(length=20)
        
        versions = []
        current_version = None
        
        for doc in documents:
            version_data = {
                "version": doc.get("version", ""),
                "description": doc.get("description", ""),
                "created_at": doc.get("created_at", "").isoformat() if doc.get("created_at") else "",
                "is_stable": doc.get("is_stable", False),
                "performance": doc.get("performance", {}),
                "rules_count": doc.get("rules_count", 0),
                "changes": doc.get("changes", [])
            }
            versions.append(version_data)
            
            if doc.get("is_current", False):
                current_version = doc.get("version")
        
        # MongoDBì— ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
        if not versions:
            logger.warning("No rule versions found in MongoDB, using default data")
            return {
                "versions": [
                    {
                        "version": "v1.0.2",
                        "description": "ê¸°ë³¸ ê·œì¹™ ì„¸íŠ¸",
                        "created_at": "2024-01-20T15:45:00",
                        "is_stable": True,
                        "performance": {
                            "avg_token_reduction": 26.2,
                            "avg_nrr": 0.958,
                            "avg_fpr": 0.994,
                            "avg_ss": 0.931
                        },
                        "rules_count": 17
                    }
                ],
                "current_version": "v1.0.2",
                "total_versions": 1
            }
        
        return {
            "versions": versions,
            "current_version": current_version or (versions[0]["version"] if versions else "v1.0.2"),
            "total_versions": len(versions)
        }
        
    except Exception as e:
        logger.error(f"Failed to fetch rule versions: {e}")
        raise HTTPException(status_code=500, detail="Failed to fetch rule versions")


@router.get("/rules/versions/{version}")
async def get_rule_version_detail(version: str):
    """íŠ¹ì • ê·œì¹™ íŒŒì¼ ë²„ì „ì˜ ìƒì„¸ ì •ë³´"""
    try:
        from app.core.database import db_manager
        
        collection = db_manager.get_collection("rules_versions")
        
        if collection is None:
            raise HTTPException(
                status_code=503, 
                detail="Database connection unavailable. Please check MongoDB connection."
            )
            return {
                "version": version,
                "description": f"ê·œì¹™ ì„¸íŠ¸ {version}",
                "created_at": "2024-01-15T12:30:00",
                "is_stable": version in ["v1.0.0", "v1.0.2"],
                "is_current": version == "v1.0.2",
                "performance": {
                    "avg_token_reduction": 24.8,
                    "avg_nrr": 0.951,
                    "avg_fpr": 0.992,
                    "avg_ss": 0.925,
                    "test_cases_passed": 1847,
                    "test_cases_total": 2000
                },
                "rules": [
                    {
                        "name": "page_number_removal",
                        "description": "í˜ì´ì§€ ë²ˆí˜¸ ì œê±°",
                        "pattern": r"(?:^|\n)\s*í˜ì´ì§€\s*\d+\s*(?:\n|$)",
                        "replacement": "\n",
                        "enabled": True,
                        "priority": 1
                    },
                    {
                        "name": "separator_removal", 
                        "description": "êµ¬ë¶„ì„  ì œê±°",
                        "pattern": r"(?:^|\n)\s*[-=]{3,}\s*(?:\n|$)",
                        "replacement": "\n",
                        "enabled": True,
                        "priority": 2
                    },
                    {
                        "name": "whitespace_normalization",
                        "description": "ê³µë°± ì •ê·œí™”",
                        "pattern": r"\s{2,}",
                        "replacement": " ",
                        "enabled": True,
                        "priority": 3
                    }
                ],
                "changes": [
                    "í˜ì´ì§€ ë²ˆí˜¸ ì •ê·œì‹ íŒ¨í„´ ê°œì„ ",
                    "êµ¬ë¶„ì„  ì œê±° ê·œì¹™ ìµœì í™”"
                ],
                "test_results": {
                    "regression_tests": "í†µê³¼ (0 ì‹¤íŒ¨)",
                    "unit_tests": "í†µê³¼ (15/15)",
                    "holdout_validation": "í†µê³¼ (NRR: 0.951)"
                }
            }
        
        # ì‹¤ì œ MongoDBì—ì„œ ì¡°íšŒ
        document = await collection.find_one({"version": version})
        
        if not document:
            logger.warning(f"Rule version {version} not found in MongoDB")
            return {
                "version": version,
                "description": f"ê·œì¹™ ì„¸íŠ¸ {version}",
                "created_at": "2024-01-15T12:30:00",
                "is_stable": version in ["v1.0.0", "v1.0.2"],
                "is_current": version == "v1.0.2",
                "performance": {
                    "avg_token_reduction": 24.8,
                    "avg_nrr": 0.951,
                    "avg_fpr": 0.992,
                    "avg_ss": 0.925,
                    "test_cases_passed": 1847,
                    "test_cases_total": 2000
                },
                "rules": [
                    {
                        "name": "page_number_removal",
                        "description": "í˜ì´ì§€ ë²ˆí˜¸ ì œê±°",
                        "pattern": r"(?:^|\n)\s*í˜ì´ì§€\s*\d+\s*(?:\n|$)",
                        "replacement": "\n",
                        "enabled": True,
                        "priority": 1
                    },
                    {
                        "name": "separator_removal", 
                        "description": "êµ¬ë¶„ì„  ì œê±°",
                        "pattern": r"(?:^|\n)\s*[-=]{3,}\s*(?:\n|$)",
                        "replacement": "\n",
                        "enabled": True,
                        "priority": 2
                    },
                    {
                        "name": "whitespace_normalization",
                        "description": "ê³µë°± ì •ê·œí™”",
                        "pattern": r"\s{2,}",
                        "replacement": " ",
                        "enabled": True,
                        "priority": 3
                    }
                ],
                "changes": [
                    "í˜ì´ì§€ ë²ˆí˜¸ ì •ê·œì‹ íŒ¨í„´ ê°œì„ ",
                    "êµ¬ë¶„ì„  ì œê±° ê·œì¹™ ìµœì í™”"
                ],
                "test_results": {
                    "regression_tests": "í†µê³¼ (0 ì‹¤íŒ¨)",
                    "unit_tests": "í†µê³¼ (15/15)",
                    "holdout_validation": "í†µê³¼ (NRR: 0.951)"
                }
            }
        
        return {
            "version": document.get("version", ""),
            "description": document.get("description", ""),
            "created_at": document.get("created_at", "").isoformat() if document.get("created_at") else "",
            "is_stable": document.get("is_stable", False),
            "is_current": document.get("is_current", False),
            "performance": document.get("performance", {}),
            "rules": document.get("rules", []),
            "changes": document.get("changes", []),
            "test_results": document.get("test_results", {})
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to fetch rule version detail: {e}")
        raise HTTPException(status_code=500, detail="Failed to fetch rule version detail")


@router.get("/cases/{case_id}/diff")
async def get_case_diff(case_id: str):
    """ì¼€ì´ìŠ¤ ì „í›„ ë¹„êµ (ì›ë³¸ vs ì „ì²˜ë¦¬ëœ ë‚´ìš©)"""
    try:
        # cases ì»¬ë ‰ì…˜ì—ì„œ ì „ì²˜ë¦¬ëœ ì¼€ì´ìŠ¤ ì¡°íšŒ
        from app.core.database import db_manager
        collection = db_manager.get_collection("cases")
        
        if collection is None:
            raise HTTPException(
                status_code=503, 
                detail="Database connection unavailable. Please check MongoDB connection."
            )
        
        # cases ì»¬ë ‰ì…˜ì—ì„œ ì „ì²˜ë¦¬ëœ ì¼€ì´ìŠ¤ ì¡°íšŒ
        from bson import ObjectId
        
        # case_idê°€ original_idì¸ì§€ processed_idì¸ì§€ í™•ì¸
        if ObjectId.is_valid(case_id):
            # ObjectIdë¡œ ì§ì ‘ ì¡°íšŒ (processed_id)
            document = await collection.find_one({"_id": ObjectId(case_id)})
        else:
            # original_idë¡œ ì¡°íšŒ
            document = await collection.find_one({"original_id": case_id})
        
        if not document:
            raise HTTPException(status_code=404, detail="Processed case not found")
        
        before_content = document.get("original_content", "")
        after_content = document.get("processed_content", "")
        
        # ê°„ë‹¨í•œ diff ê³„ì‚°
        before_lines = before_content.split('\n')
        after_lines = after_content.split('\n')
        
        lines_removed = len(before_lines) - len(after_lines)
        characters_removed = len(before_content) - len(after_content)
        token_reduction = document.get("token_reduction_percent", 0)
        
        return {
            "case_id": case_id,
            "original_id": document.get("original_id", ""),
            "processed_id": str(document.get("_id")),
            "before_content": before_content,
            "after_content": after_content,
            "diff_html": f"<div class='diff-summary'>ì œê±°ëœ ë¼ì¸: {lines_removed}ê°œ, ì œê±°ëœ ë¬¸ì: {characters_removed}ê°œ</div>",
            "summary": {
                "lines_removed": max(0, lines_removed),
                "lines_added": max(0, -lines_removed),
                "characters_removed": max(0, characters_removed),
                "characters_added": max(0, -characters_removed),
                "token_reduction_percent": token_reduction
            },
            "processing_info": {
                "rules_version": document.get("rules_version", ""),
                "processing_mode": document.get("processing_mode", ""),
                "quality_score": document.get("quality_score", 0),
                "created_at": document.get("created_at", "")
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get case diff: {e}")
        raise HTTPException(status_code=500, detail="Failed to get case diff")


# ================================
# ê³ ê¸‰ ì‚¬ì‹¤ ì¶”ì¶œ ì‹œìŠ¤í…œ
# ================================

def _extract_factual_content_only(content: str) -> str:
    """
    ìˆœìˆ˜ ì‚¬ì‹¤ë§Œ ì¶”ì¶œí•˜ê³  ë²•ë¦¬/íŒë‹¨ ë‚´ìš©ì„ ì œê±°í•˜ëŠ” ê³ ê¸‰ ì „ì²˜ë¦¬
    """
    if not content:
        return content
    
    logger.info("ğŸ” ê³ ê¸‰ ì‚¬ì‹¤ ì¶”ì¶œ ì‹œì‘")
    
    # 1ë‹¨ê³„: ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ë¦¬
    cleaned_text = _clean_text_noise(content)
    
    # 2ë‹¨ê³„: ì„¹ì…˜ êµ¬ë¶„ ë° ì‚¬ì‹¤ ë¸”ë¡ ì„ ë³„
    fact_sections = _identify_fact_sections(cleaned_text)
    
    # 3ë‹¨ê³„: ë¬¸ì¥ ë‹¨ìœ„ ìŠ¤ì½”ì–´ë§ ë° í•„í„°ë§
    fact_sentences = _extract_fact_sentences_only(fact_sections)
    
    # 4ë‹¨ê³„: ë²•ë¦¬/íŒë‹¨ ë¬¸ì¥ ì œê±°
    pure_fact_sentences = _remove_legal_reasoning_sentences(fact_sentences)
    
    # 5ë‹¨ê³„: ìµœì¢… ì¡°ë¦½ ë° ì •ê·œí™”
    final_content = _assemble_and_normalize_facts(pure_fact_sentences)
    
    # ìµœì¢… ì•ˆì „ì¥ì¹˜: ê²°ê³¼ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ì›ë³¸ì˜ ì¼ë¶€ ì‚¬ìš©
    if len(final_content) < 200:
        logger.error(f"ğŸš¨ ì „ì²˜ë¦¬ ê²°ê³¼ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤: {len(final_content)}ì. ì›ë³¸ ì¼ë¶€ ì‚¬ìš©...")
        # ì›ë³¸ì—ì„œ ì²˜ìŒ 2000ì ì •ë„ ì‚¬ìš© (ë…¸ì´ì¦ˆ ì œê±°ë§Œ ì ìš©)
        fallback_content = _clean_text_noise(content)
        if len(fallback_content) > 2000:
            final_content = fallback_content[:2000] + "..."
        else:
            final_content = fallback_content
        logger.warning(f"ğŸ”§ í´ë°± ì ìš©: {len(final_content)}ì")
    
    logger.info(f"âœ… ì‚¬ì‹¤ ì¶”ì¶œ ì™„ë£Œ: {len(content)}ì â†’ {len(final_content)}ì")
    return final_content


def _clean_text_noise(content: str) -> str:
    """ê¸°ë³¸ í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ ì œê±°"""
    text = content
    
    # UI/ë©”ë‰´ ì œê±° íŒ¨í„´ë“¤ (ì‹¤ì œ ì›ë³¸ ë°ì´í„° ê¸°ì¤€)
    noise_patterns = [
        r'íŒë¡€ìƒì„¸\s*ì €ì¥\s*ì¸ì‡„\s*ë³´ê´€\s*ì „ìíŒ©ìŠ¤\s*ê³µìœ \s*í™”ë©´ë‚´\s*ê²€ìƒ‰\s*ì¡°íšŒ\s*ë‹«ê¸°',
        r'ì¬íŒê²½ê³¼\s*.*?\s*ì°¸ì¡°íŒë¡€\s*\d+\s*ê±´\s*ì¸ìš©íŒë¡€\s*\d+\s*ê±´',
        r'PDFë¡œ\s*ë³´ê¸°\s*ì•ˆë‚´.*?ì¶œë ¥ì„\s*í•˜ì‹¤\s*ìˆ˜\s*ìˆìŠµë‹ˆë‹¤\.',
        r'ìƒì„¸ë‚´ìš©\s*ì•ˆì—\s*ìˆëŠ”\s*í‘œë‚˜\s*ë„í˜•.*?ì›ë³¸\s*ê·¸ëŒ€ë¡œ\s*ì¶œë ¥ì„\s*í•˜ì‹¤\s*ìˆ˜\s*ìˆìŠµë‹ˆë‹¤\.',
        r'Tip\d+\..*?ë‹«ê¸°',
        r'ìœ ì‚¬ë¬¸ì„œ\s*\d+\s*ê±´.*?íƒœê·¸\s*í´ë¼ìš°ë“œ.*?ë‹«ê¸°',
        r'ìœ ì‚¬ìœ¨\s*\d+%.*?100%',
        r'íƒœê·¸\s*í´ë¼ìš°ë“œ\s*ìì„¸íˆë³´ê¸°.*?ê²€ìƒ‰í•˜ê¸°',
        r'ê²€ìƒ‰í•˜ê¸°\s*í†µí•©ê²€ìƒ‰\s*ê²€ìƒ‰í•˜ê¸°',
        r'#\w+(?:\s*#\w+)*',  # íƒœê·¸ë“¤ (#ëŒ€í‘œ #ì´ì‚¬ #íŠ¹í—ˆ ë“±)
        r'êµ­ìŠ¹\s*ê´‘ì£¼ì§€ë°©ë²•ì›-\d{4}-êµ¬í•©-\d+',
        r'ê·€ì†ë…„ë„\s*:\s*\d{4}\s*ì‹¬ê¸‰\s*:\s*\d+ì‹¬\s*ìƒì‚°ì¼ì\s*:\s*\d{4}\.\d{2}\.\d{2}\.\s*ì§„í–‰ìƒíƒœ\s*:\s*ì§„í–‰ì¤‘'
    ]
    
    for pattern in noise_patterns:
        text = re.sub(pattern, '', text, flags=re.DOTALL | re.IGNORECASE)
    
    # ì¤„ë°”ê¿ˆÂ·ê³µë°± ì •ê·œí™”
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\s*[,.]\s*', ', ', text)
    
    return text.strip()


def _identify_fact_sections(text: str) -> str:
    """ì‚¬ì‹¤ ê´€ë ¨ ì„¹ì…˜ë§Œ ì‹ë³„í•˜ì—¬ ì¶”ì¶œ"""
    # ì‚¬ì‹¤ ì„¹ì…˜ ì‹œì‘ íŒ¨í„´
    fact_start_patterns = [
        r'ì‚¬ì‹¤\s*ê´€ê³„', r'ì¸ì •\s*ì‚¬ì‹¤', r'ì‚¬ì‹¤', r'ì²˜ë¶„ì˜\s*ê²½ìœ„', 
        r'ì‚¬ê±´', r'ì¬íŒ\s*ê²½ê³¼', r'ë²”ì£„\s*ì‚¬ì‹¤'
    ]
    
    # ì‚¬ì‹¤ ì„¹ì…˜ ì¢…ë£Œ íŒ¨í„´ (ë²•ë¦¬/íŒë‹¨ ì‹œì‘)
    fact_end_patterns = [
        r'ì´ìœ ', r'íŒë‹¨', r'ê´€ë ¨\s*ë²•ë¦¬', r'ë²•ë¦¬', r'ì£¼\s*ë¬¸', 
        r'ì£¼ë¬¸', r'ê²°ë¡ ', r'ìš”ì§€', r'ì°¸ì¡°', r'ë³„ì§€'
    ]
    
    # ì‚¬ì‹¤ êµ¬ê°„ ì°¾ê¸°
    start_pos = 0
    for pattern in fact_start_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            start_pos = match.start()
            logger.info(f"ğŸ“ ì‚¬ì‹¤ ì„¹ì…˜ ì‹œì‘: {match.group(0)} at {start_pos}")
            break
    
    # ì‚¬ì‹¤ êµ¬ê°„ ì¢…ë£Œì  ì°¾ê¸°
    end_pos = len(text)
    for pattern in fact_end_patterns:
        match = re.search(pattern, text[start_pos:], re.IGNORECASE)
        if match:
            end_pos = start_pos + match.start()
            logger.info(f"ğŸ›‘ ì‚¬ì‹¤ ì„¹ì…˜ ì¢…ë£Œ: {match.group(0)} at {end_pos}")
            break
    
    fact_section = text[start_pos:end_pos]
    logger.info(f"ğŸ“ ì‚¬ì‹¤ ì„¹ì…˜ ì¶”ì¶œ: {len(fact_section)}ì")
    
    return fact_section if len(fact_section) > 100 else text[:len(text)//2]  # ë„ˆë¬´ ì§§ìœ¼ë©´ ì•ë¶€ë¶„ ì‚¬ìš©


def _extract_fact_sentences_only(text: str) -> List[str]:
    """ì‚¬ì‹¤ ë¬¸ì¥ë§Œ ì¶”ì¶œ (ìŠ¤ì½”ì–´ë§ ê¸°ë°˜)"""
    # ë¬¸ì¥ ë¶„í• 
    sentences = re.split(r'[.!?]\s+', text)
    
    fact_sentences = []
    for sentence in sentences:
        if len(sentence.strip()) < 20:
            continue
            
        # ë¬¸ì¥ ìŠ¤ì½”ì–´ë§
        score = _score_sentence_factuality(sentence)
        
        # ì ìˆ˜ê°€ -1 ì´ìƒì¸ ë¬¸ì¥ ì„ íƒ (ë” ê´€ëŒ€í•œ ê¸°ì¤€)
        if score >= -1:
            fact_sentences.append(sentence.strip())
            logger.debug(f"âœ… ì‚¬ì‹¤ ë¬¸ì¥ (ì ìˆ˜ {score}): {sentence[:50]}...")
        else:
            logger.debug(f"âŒ ì œì™¸ ë¬¸ì¥ (ì ìˆ˜ {score}): {sentence[:50]}...")
    
    # ì•ˆì „ì¥ì¹˜: ê²°ê³¼ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì›ë³¸ì˜ ì¼ë¶€ë¼ë„ ì‚¬ìš©
    if len(fact_sentences) < 5:
        logger.warning(f"âš ï¸ ì‚¬ì‹¤ ë¬¸ì¥ì´ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤ ({len(fact_sentences)}ê°œ). ì›ë³¸ ë¬¸ì¥ ì¼ë¶€ ì¶”ê°€...")
        # ì›ë³¸ì—ì„œ ìµœì†Œí•œì˜ ë¬¸ì¥ë“¤ ì¶”ê°€ (ê¸¸ì´ ê¸°ì¤€)
        all_sentences = [s.strip() for s in sentences if len(s.strip()) > 30]
        fact_sentences.extend(all_sentences[:10])  # ìµœëŒ€ 10ê°œ ì¶”ê°€
        fact_sentences = list(set(fact_sentences))  # ì¤‘ë³µ ì œê±°
    
    logger.info(f"ğŸ“Š ì‚¬ì‹¤ ë¬¸ì¥ ì¶”ì¶œ: {len(sentences)} â†’ {len(fact_sentences)}ê°œ")
    return fact_sentences


def _score_sentence_factuality(sentence: str) -> int:
    """ë¬¸ì¥ì˜ ì‚¬ì‹¤ì„± ì ìˆ˜ ê³„ì‚° (ì¡°ì •ëœ ë²„ì „)"""
    score = 0
    
    # ì‚¬ì‹¤ ì‹ í˜¸ (+1ì ì”©)
    fact_signals = {
        'dates': r'\d{4}[.\-/ë…„]\s*\d{1,2}[.\-/ì›”]\s*\d{1,2}[.\-/ì¼]?',
        'amounts': r'\d{1,3}(?:,\d{3})*(?:ì›|ë§Œì›|ì–µì›)',
        'parties': r'ì›ê³ |í”¼ê³ |ì‹ ì²­ì¸|í”¼ì‹ ì²­ì¸|ì¡°ì„¸ì‹¬íŒì›|ì„¸ë¬´ì„œì¥|ì£¼ì‹íšŒì‚¬|ë²•ì¸',
        'actions': r'ê³„ì•½|ì¶œì›|ë“±ë¡|ì–‘ë„|ì´ì „ë“±ë¡|ìƒê³„|ê³„ìƒ|ì›ì²œì§•ìˆ˜|ë¶€ê³¼|í†µì§€|ì œê¸°|ê¸°ê°|ì‘ì„±|ì œì¶œ|ë§¤ìˆ˜|ë§¤ë„|ë¶„ì–‘|ì†¡ê¸ˆ|ì§€ê¸‰|ë‚©ë¶€|ì‹ ê³ |ìˆ˜ì£¼|ê³µì‚¬',
        'evidence': r'ê³„ì•½ì„œ|ì‚¬ì—…ê³„íšì„œ|ì¬ë¬´ì œí‘œ|í˜¸ì¦|ë¬¸ë‹µì„œ|í™•ì¸ì„œ|ì¦ë¹™|í†µì¥|ì˜ìˆ˜ì¦',
        'case_context': r'ì‚¬ê±´|ì²˜ë¶„|ê°€ë§¹|ë§¤ì¶œ|ê±°ë˜|ë‹¹ì‚¬ì|ì¤‘êµ­|ê°€ë§¹ì '  # ì‚¬ê±´ ë§¥ë½ í‚¤ì›Œë“œ ì¶”ê°€
    }
    
    for signal_type, pattern in fact_signals.items():
        matches = len(re.findall(pattern, sentence))
        score += matches  # ë§¤ì¹˜ ê°œìˆ˜ë§Œí¼ ì ìˆ˜ ì¶”ê°€
    
    # ì‚¬ê±´ ì‹œì‘ íŒ¨í„´ íŠ¹ë³„ ê°€ì‚° (+2ì ìœ¼ë¡œ ê°ì†Œ)
    case_start_patterns = [
        r'^ì›ê³ ëŠ”?\s*\d{4}ë…„?.*(?:ê³„ì•½|ë§¤ìˆ˜|ì·¨ë“|ê³µì‚¬|ì‹œê³µ)',
        r'^í”¼ê³ ëŠ”?\s*\d{4}ë…„?.*(?:ì²˜ë¶„|ë¶€ê³¼|í†µì§€)',
        r'^ì‹ ì²­ì¸ì€?\s*.*(?:ì‹ ì²­|ì œê¸°|ìš”êµ¬)',
        r'^.*?ëŠ”?\s*\d{4}\.\d{1,2}\.\d{1,2}\.?ë¶€í„°.*?(?:ê³µì‚¬|ì‹œê³µ|ì‘ì—…|ê·¼ë¬´|ê³„ì•½)',
        r'.*?ìˆ˜ì£¼.*?ëŒ€ê¸ˆ.*?ë°›ì•˜ë‹¤',  # "ì¤‘êµ­ ê°€ë§¹ì  ê³µì‚¬ë¥¼ ìˆ˜ì£¼í•˜ê³  ëŒ€ê¸ˆì„ ë°›ì•˜ë‹¤"
        r'.*?í™•ì¸ì„œ.*?ì‘ì„±.*?ì œì¶œ'   # "í™•ì¸ì„œë¥¼ ì‘ì„±í•´ì„œ ì œì¶œí–ˆë‹¤"
    ]
    
    for pattern in case_start_patterns:
        if re.search(pattern, sentence):
            score += 2  # 3ì ì—ì„œ 2ì ìœ¼ë¡œ ê°ì†Œ
            break
    
    # ë²•ë¦¬/íŒë‹¨ ì‹ í˜¸ (-2ì ìœ¼ë¡œ ê°ì†Œ, ëœ ê³µê²©ì )
    legal_signals = {
        'judgments': r'íƒ€ë‹¹í•˜ë‹¤|ì •ë‹¹í•˜ë‹¤|ë¶€ë‹¹í•˜ë‹¤|ë³¼\s*ìˆ˜\s*ì—†ë‹¤|ë³´ì•„ì•¼\s*í•œë‹¤|ì¸ì •ëœë‹¤|íŒë‹¨ëœë‹¤|ë¼\s*í• \s*ê²ƒ',
        'evaluations': r'ë”\s*ë†’ë‹¤ê³ \s*ë³´ì¸ë‹¤|ë‚®ë‹¤ê³ \s*ë³´ì¸ë‹¤|ë‹¨ì •í•˜ê¸°\s*ì–´ë µë‹¤|ì¶”ì •ëœë‹¤|ì¶”ë¡ ëœë‹¤|ìƒê°ëœë‹¤',
        'assessments': r'ê°€ëŠ¥ì„±ì´?\s*ìˆë‹¤|ìˆë‹¤ê³ \s*í• \s*ìˆ˜\s*ìˆë‹¤|ì—†ë‹¤ê³ \s*í• \s*ìˆ˜\s*ì—†ë‹¤|ì—¬ê²¨ì§„ë‹¤|ë³´ì—¬ì§„ë‹¤',
        'conclusions': r'ì£¼\s*ë¬¸|ì²­êµ¬.*(?:ê¸°ê°|ì¸ìš©|ê°í•˜)',
        'legal_refs': r'ê´€ë ¨\s*ë²•ë¦¬|ë²•ë¦¬|ëŒ€ë²•ì›.*ì„ ê³ .*íŒê²°|íŒì‹œ',
        'statutes': r'ì œ\d+ì¡°(?!.*(ì²˜ë¶„|í†µì§€|ê³„ì•½|ì–‘ë„|ì´ì „ë“±ë¡))'
    }
    
    for signal_type, pattern in legal_signals.items():
        if re.search(pattern, sentence):
            score -= 2  # -3ì ì—ì„œ -2ì ìœ¼ë¡œ ì™„í™”
    
    # ìµœì†Œ ì ìˆ˜ ë³´ì¥ (ë„ˆë¬´ ë§ì´ ì œê±°ë˜ì§€ ì•Šë„ë¡)
    if score < -1:
        score = -1
    
    return score


def _remove_legal_reasoning_sentences(sentences: List[str]) -> List[str]:
    """ë²•ë¦¬/íŒë‹¨ ë¬¸ì¥ ì™„ì „ ì œê±°"""
    # ì¦‰ì‹œ ì œê±° íŒ¨í„´ (í•˜ë“œ í•„í„°)
    immediate_drop_patterns = [
        r'^ì£¼\s*ë¬¸', r'^ì´ìœ ', r'^íŒë‹¨', r'^ê´€ë ¨\s*ë²•ë¦¬', r'^ë²•ë¦¬',
        r'^ìš”ì§€', r'^ìƒì„¸ë‚´ìš©', r'^ë¶™ì„', r'^PDFë¡œ\s*ë³´ê¸°',
        r'ì²­êµ¬ë¥¼\s*(ê¸°ê°|ê°í•˜|ì¸ìš©)', r'ëŒ€ë²•ì›.*ì„ ê³ .*íŒê²°.*ì°¸ì¡°',
        r'íŒê²°\s*ì„ ê³ ', r'ë³€ë¡ \s*ì¢…ê²°.*íŒê²°\s*ì„ ê³ ',
        r'^ê·¸\s*ë°–ì˜?\s*ì—¬ëŸ¬\s*ì‚¬ì •ì„?\s*ì‚´í´ë³´ì•„?ë„?',
        r'^ì´ìƒì˜?\s*ì‚¬ì •ì„?\s*ì¢…í•©í•˜ë©´?',
        r'^ìœ„ì™€?\s*ê°™ì€\s*ì‚¬ì •ì„?\s*ê³ ë ¤í•˜ë©´?'
    ]
    
    filtered_sentences = []
    for sentence in sentences:
        should_drop = False
        
        # ì¦‰ì‹œ ì œê±° íŒ¨í„´ ê²€ì‚¬
        for pattern in immediate_drop_patterns:
            if re.search(pattern, sentence, re.IGNORECASE):
                logger.debug(f"ğŸš« ë²•ë¦¬ ë¬¸ì¥ ì œê±°: {sentence[:50]}...")
                should_drop = True
                break
        
        if not should_drop:
            filtered_sentences.append(sentence)
    
    logger.info(f"âš–ï¸ ë²•ë¦¬ ì œê±°: {len(sentences)} â†’ {len(filtered_sentences)}ê°œ ë¬¸ì¥")
    return filtered_sentences


def _assemble_and_normalize_facts(sentences: List[str]) -> str:
    """ì‚¬ì‹¤ ë¬¸ì¥ë“¤ì„ ì¡°ë¦½í•˜ê³  ì •ê·œí™”"""
    if not sentences:
        return ""
    
    # ë¬¸ì¥ ì¡°ë¦½
    text = '. '.join(sentences)
    
    # ë‚ ì§œ ì •ê·œí™” (YYYY.MM.DD í†µì¼)
    text = re.sub(r'(\d{4}),\s*(\d{1,2}),\s*(\d{1,2})', lambda m: f"{m.group(1)}.{int(m.group(2)):02d}.{int(m.group(3)):02d}", text)
    text = re.sub(r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼', lambda m: f"{m.group(1)}.{int(m.group(2)):02d}.{int(m.group(3)):02d}", text)
    text = re.sub(r'(\d{4})[/\-]\s*(\d{1,2})[/\-]\s*(\d{1,2})', lambda m: f"{m.group(1)}.{int(m.group(2)):02d}.{int(m.group(3)):02d}", text)
    
    # ê¸ˆì•¡ ì •ê·œí™” (ê³µë°± ì œê±°)
    text = re.sub(r'(\d+)\s*,\s*(\d{3})', r'\1,\2', text)
    text = re.sub(r'(\d+)\s*(ì›|ë§Œì›|ì–µì›)', r'\1\2', text)
    
    # íŒë¡€ ì°¸ì¡° ì •ë³´ ì œê±°
    text = re.sub(r'\(ëŒ€ë²•ì› \d{4}\. \d{1,2}\. \d{1,2}\. ì„ ê³  \d+[ê°€-í£]+\d+ íŒê²°[^)]*\)', '', text)
    
    # ì†Œì†¡ëŒ€ë¦¬ì¸ ì •ë³´ ì œê±°
    text = re.sub(r'\(ì†Œì†¡ëŒ€ë¦¬ì¸ [^)]+\)', '', text)
    
    # ì‚¬ê±´ë²ˆí˜¸ ìµëª…í™”
    text = re.sub(r'\d{4}[ê°€-í£]+\d+', 'â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹', text)
    
    # ê³µë°± ì •ê·œí™”
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\.\s*\.', '.', text)
    
    # ê¸¸ì´ ì¡°ì • (800-2000ì ëª©í‘œë¡œ ì™„í™”)
    if len(text) > 2000:
        # ë„ˆë¬´ ê¸¸ë©´ ì¤‘ìš”í•œ ë¬¸ì¥ë“¤ë§Œ ì„ ë³„
        important_sentences = _select_most_important_sentences(sentences, 1800)
        text = '. '.join(important_sentences)
    elif len(text) < 500:
        logger.warning(f"âš ï¸ ì‚¬ì‹¤ ì¶”ì¶œ ê²°ê³¼ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤: {len(text)}ì")
    elif len(text) < 800:
        logger.info(f"ğŸ“ ì‚¬ì‹¤ ì¶”ì¶œ ê²°ê³¼ê°€ ì§§ì§€ë§Œ í—ˆìš© ë²”ìœ„: {len(text)}ì")
    
    return text.strip()


def _select_most_important_sentences(sentences: List[str], target_length: int) -> List[str]:
    """ê°€ì¥ ì¤‘ìš”í•œ ë¬¸ì¥ë“¤ ì„ ë³„"""
    # ë¬¸ì¥ë³„ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
    scored_sentences = []
    for sentence in sentences:
        importance = 0
        
        # ë‚ ì§œ í¬í•¨: +3ì 
        if re.search(r'\d{4}\.\d{2}\.\d{2}', sentence):
            importance += 3
        
        # ê¸ˆì•¡ í¬í•¨: +3ì 
        if re.search(r'[0-9,]+ì›', sentence):
            importance += 3
        
        # ë‹¹ì‚¬ì í¬í•¨: +2ì 
        if re.search(r'ì›ê³ |í”¼ê³ |ëŒ€í‘œì´ì‚¬|ì„¸ë¬´ì„œì¥', sentence):
            importance += 2
        
        # í•µì‹¬ í–‰ìœ„ í¬í•¨: +2ì 
        if re.search(r'ì¶œì›|ë“±ë¡|ì–‘ë„|ì´ì „ë“±ë¡|ìƒê³„|ê³„ìƒ|ì›ì²œì§•ìˆ˜|ë¶€ê³¼|í†µì§€|ì œê¸°|ì‘ì„±|ì œì¶œ', sentence):
            importance += 2
        
        # ì‚¬ê±´ë²ˆí˜¸ í¬í•¨: +1ì 
        if re.search(r'\d{4}[ê°€ë‚˜ë‹¤ë¼ë§ˆë°”ì‚¬ì•„ìì°¨ì¹´íƒ€íŒŒí•˜][ê°€-í£]+\d+', sentence):
            importance += 1
        
        scored_sentences.append((sentence, importance))
    
    # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    scored_sentences.sort(key=lambda x: x[1], reverse=True)
    
    # ëª©í‘œ ê¸¸ì´ê¹Œì§€ ë¬¸ì¥ ì„ íƒ
    selected = []
    current_length = 0
    
    for sentence, score in scored_sentences:
        if current_length + len(sentence) <= target_length:
            selected.append(sentence)
            current_length += len(sentence)
        elif current_length < target_length * 0.8:  # 80% ë¯¸ë§Œì´ë©´ ê°•ì œ ì¶”ê°€
            # ë¬¸ì¥ì„ ì˜ë¼ì„œë¼ë„ ì¶”ê°€
            remaining = target_length - current_length
            if remaining > 100:
                truncated = sentence[:remaining-3] + "..."
                selected.append(truncated)
            break
    
    return selected


@router.post("/rules/initialize")
async def initialize_dsl_rules():
    """DSL ê·œì¹™ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    try:
        from app.services.dsl_rules import dsl_manager
        
        print("ğŸ”§ DEBUG: DSL ê·œì¹™ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
        logger.info("DSL ê·œì¹™ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
        
        # ê°•ì œë¡œ ê¸°ë³¸ ê·œì¹™ ìƒì„± ë° ì €ì¥
        dsl_manager._create_default_rules()
        dsl_manager.save_rules()
        
        # ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
        performance_report = dsl_manager.get_performance_report()
        
        print(f"ğŸ”§ DEBUG: DSL ê·œì¹™ ì´ˆê¸°í™” ì™„ë£Œ - {performance_report['total_rules']}ê°œ ê·œì¹™ ìƒì„±")
        logger.info(f"DSL ê·œì¹™ ì´ˆê¸°í™” ì™„ë£Œ: {performance_report}")
        
        return {
            "status": "success",
            "message": "DSL ê·œì¹™ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤",
            "performance_report": performance_report,
            "rules_file": str(dsl_manager.rules_file)
        }
        
    except Exception as e:
        error_msg = f"DSL ê·œì¹™ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}"
        print(f"ğŸ”§ ERROR: {error_msg}")
        logger.error(error_msg)
        raise HTTPException(status_code=500, detail=error_msg)


@router.get("/rules/dsl/status")
async def get_dsl_status():
    """DSL ê·œì¹™ ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
    try:
        from app.services.dsl_rules import dsl_manager
        from app.services.auto_patch_engine import auto_patch_engine
        
        # DSL ë§¤ë‹ˆì € ìƒíƒœ
        performance_report = dsl_manager.get_performance_report()
        
        # íŒ¨ì¹˜ íˆìŠ¤í† ë¦¬
        patch_history = auto_patch_engine.get_patch_history()
        
        return {
            "dsl_system": {
                "status": "active",
                "rules_file": str(dsl_manager.rules_file),
                "rules_file_exists": dsl_manager.rules_file.exists(),
                "performance_report": performance_report
            },
            "auto_patch": {
                "status": "active", 
                "patch_count": len(patch_history),
                "recent_patches": patch_history[-5:] if patch_history else []
            }
        }
        
    except Exception as e:
        logger.error(f"DSL ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"DSL ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
